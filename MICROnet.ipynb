{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICROnet Training - Comprehensive Model Comparison\n",
    "\n",
    "This notebook trains 10 different microstructure prediction models:\n",
    "\n",
    "## CNN-LSTM Models:\n",
    "1. CNN-LSTM seq=3, MSE loss\n",
    "2. CNN-LSTM seq=4, MSE loss\n",
    "3. CNN-LSTM seq=4, Combined loss (T_solidus=1560, T_liquidus=1620)\n",
    "4. CNN-LSTM seq=4, Combined loss (T_solidus=1530, T_liquidus=1650)\n",
    "5. CNN-LSTM seq=4, Combined loss (T_solidus=1500, T_liquidus=1680)\n",
    "\n",
    "## PredRNN Models:\n",
    "6. PredRNN seq=3, MSE loss\n",
    "7. PredRNN seq=4, MSE loss\n",
    "8. PredRNN seq=4, Combined loss (T_solidus=1560, T_liquidus=1620)\n",
    "9. PredRNN seq=4, Combined loss (T_solidus=1530, T_liquidus=1650)\n",
    "10. PredRNN seq=4, Combined loss (T_solidus=1500, T_liquidus=1680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from lasernet.micronet.train.trainer import (\n",
    "    get_device,\n",
    "    train_microstructure,\n",
    "    load_model_and_predict,\n",
    "    save_prediction_visualization,\n",
    "    save_solidification_mask_visualization,\n",
    ")\n",
    "from lasernet.micronet.model.MicrostructureCNN_LSTM import MicrostructureCNN_LSTM\n",
    "from lasernet.micronet.model.MicrostructurePredRNN import MicrostructurePredRNN\n",
    "from lasernet.micronet.model.losses import CombinedLoss\n",
    "from lasernet.micronet.utils import plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds for Reproducibility\n",
    "\n",
    "Set all random seeds to ensure reproducible results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set Python random seed\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set NumPy random seed\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set PyTorch random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU setups\n",
    "\n",
    "# Configure PyTorch to use deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment variable for additional determinism\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED} for reproducibility\")\n",
    "print(\"  ✓ Python random\")\n",
    "print(\"  ✓ NumPy\")\n",
    "print(\"  ✓ PyTorch (CPU and CUDA)\")\n",
    "print(\"  ✓ cuDNN deterministic mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configuration\n",
    "DEVICE = get_device()\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 15\n",
    "OUTPUT_DIR = Path(\"MICROnet_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Prediction settings\n",
    "PRED_TIMESTEP = 23\n",
    "PRED_SLICE = 47\n",
    "PLANE = \"xz\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Prediction settings: timestep={PRED_TIMESTEP}, slice={PRED_SLICE}, plane={PLANE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load datasets using fast loading from preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (same approach as notebook.ipynb)\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "from pathlib import Path as PathLib\n",
    "from lasernet.micronet.dataset import MicrostructureSequenceDataset\n",
    "from lasernet.micronet.dataset.fast_loading import FastMicrostructureSequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "SEQ_LENGTH = 4  # Default sequence length for dataset creation\n",
    "SPLIT_RATIO = \"12,6,6\"\n",
    "\n",
    "# Parse split ratios\n",
    "split_ratios = list(map(int, SPLIT_RATIO.split(\",\")))\n",
    "train_ratio = split_ratios[0] / sum(split_ratios)\n",
    "val_ratio = split_ratios[1] / sum(split_ratios)\n",
    "test_ratio = split_ratios[2] / sum(split_ratios)\n",
    "\n",
    "# Check if preprocessed files are available for fast loading\n",
    "blackhole = os.environ.get(\"BLACKHOLE\")\n",
    "if not blackhole:\n",
    "    raise ValueError(\"BLACKHOLE environment variable not set. Please set it in the makefile or shell.\")\n",
    "\n",
    "print(f\"BLACKHOLE directory: {blackhole}\")\n",
    "\n",
    "processed_dir = PathLib(blackhole) / \"processed\" / \"data\"\n",
    "required_files = [\"coordinates.pt\", \"microstructure.pt\", \"temperature.pt\"]\n",
    "fast_loading_available = all((processed_dir / f).exists() for f in required_files)\n",
    "\n",
    "if fast_loading_available:\n",
    "    print(\"✓ Preprocessed files found - using fast loading\")\n",
    "    DatasetClass = FastMicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "else:\n",
    "    print(\"⚠ Preprocessed files not found - using standard loading\")\n",
    "    DatasetClass = MicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"preload\": True,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "\n",
    "train_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"train\"})\n",
    "val_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"val\"})\n",
    "test_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"test\"})\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples:   {len(val_dataset)}\")\n",
    "print(f\"  Test samples:  {len(test_dataset)}\")\n",
    "\n",
    "# Show sample dimensions\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample dimensions:\")\n",
    "print(f\"  Context temp:  {sample['context_temp'].shape}\")\n",
    "print(f\"  Context micro: {sample['context_micro'].shape}\")\n",
    "print(f\"  Future temp:   {sample['future_temp'].shape}\")\n",
    "print(f\"  Target micro:  {sample['target_micro'].shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "Define all 10 model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all model configurations\n",
    "MODEL_CONFIGS = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"0_MICROnet_cnn_lstm_UNet_seq4_CombLoss_T1560-1620\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"use_skip_connections\": True,  # U-Net skip connections enabled\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"1_MICROnet_cnn_lstm_seq4_CombLoss_T1500-1680\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"2_MICROnet_cnn_lstm_seq4_CombLoss_T1500-1680\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"3_MICROnet_cnn_lstm_UNet_seq4_CombLoss_T1560-1620\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"use_skip_connections\": True,  # U-Net skip connections enabled\n",
    "    },\n",
    "]\n",
    "    # CNN-LSTM models\n",
    "\"\"\"\n",
    "{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"1_MICROnet_cnn_lstm_seq3_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"2_MICROnet_cnn_lstm_seq4_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"3_MICROnet_cnn_lstm_seq4_CombLoss_T1560-1620\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"4_MICROnet_cnn_lstm_seq4_CombLoss_T1530-1650\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\"\"\"\n",
    "\n",
    "    # PredRNN models\n",
    "\"\"\" {\n",
    "        \"id\": 6,\n",
    "        \"name\": \"6_MICROnet_predrnn_seq3_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"name\": \"7_MICROnet_predrnn_seq4_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"name\": \"8_MICROnet_predrnn_seq4_CombLoss_T1560-1620\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"name\": \"9_MICROnet_predrnn_seq4_CombLoss_T1530-1650\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"name\": \"10_MICROnet_predrnn_seq4_CombLoss_T1500-1680\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"use_skip_connections\": False,\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Configured {len(MODEL_CONFIGS)} models for training\")\n",
    "for cfg in MODEL_CONFIGS:\n",
    "    skip_str = \" (U-Net)\" if cfg.get('use_skip_connections', False) else \"\"\n",
    "    print(f\"  {cfg['id']:2d}. {cfg['name']}{skip_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train each model. Skip if already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Train a single model based on configuration.\n",
    "    Skip if already trained.\n",
    "    \"\"\"\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "    # Check if already trained\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Model {config['id']}: {config['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"✓ Model already trained. Skipping training.\")\n",
    "\n",
    "        # Load history if available\n",
    "        history_path = run_dir / \"history.json\"\n",
    "        if history_path.exists():\n",
    "            with open(history_path, 'r') as f:\n",
    "                history = json.load(f)\n",
    "        else:\n",
    "            history = None\n",
    "\n",
    "        return {\"status\": \"skipped\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "    # Create directories\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model {config['id']}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Model type: {config['model_type']}\")\n",
    "    print(f\"  Sequence length: {config['seq_length']}\")\n",
    "    print(f\"  Loss type: {config['loss_type']}\")\n",
    "    print(f\"  Skip connections: {config.get('use_skip_connections', False)}\")\n",
    "    if config['loss_type'] == 'combined':\n",
    "        print(f\"  T_solidus: {config['t_solidus']} K\")\n",
    "        print(f\"  T_liquidus: {config['t_liquidus']} K\")\n",
    "\n",
    "    # FIX: Create dataloaders with the CORRECT sequence length for THIS model\n",
    "    print(f\"\\n  Creating datasets with sequence length {config['seq_length']}...\")\n",
    "    if fast_loading_available:\n",
    "        model_train_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "    else:\n",
    "        model_train_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "\n",
    "    model_train_loader = DataLoader(model_train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    model_val_loader = DataLoader(model_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"  ✓ Datasets created: train={len(model_train_dataset)}, val={len(model_val_dataset)}\")\n",
    "\n",
    "    # Create model with skip connections option\n",
    "    use_skip_connections = config.get('use_skip_connections', False)\n",
    "\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        model = MicrostructureCNN_LSTM(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "    else:  # predrnn\n",
    "        model = MicrostructurePredRNN(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    param_count = model.count_parameters()\n",
    "    print(f\"  Parameters: {param_count:,}\")\n",
    "\n",
    "    # Create loss function\n",
    "    if config['loss_type'] == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:  # combined\n",
    "        criterion = CombinedLoss(\n",
    "            solidification_weight=0.7,\n",
    "            global_weight=0.3,\n",
    "            T_solidus=config['t_solidus'],\n",
    "            T_liquidus=config['t_liquidus'],\n",
    "            weight_type=\"gaussian\",\n",
    "            weight_scale=0.1,\n",
    "            base_weight=0.1,\n",
    "            return_components=True,  # Enable component tracking for visualization\n",
    "        )\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        \"model\": {\n",
    "            \"name\": config['model_type'],\n",
    "            \"parameters\": param_count,\n",
    "            \"sequence_length\": config['seq_length'],\n",
    "            \"use_skip_connections\": use_skip_connections,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"loss_type\": config['loss_type'],\n",
    "        },\n",
    "        \"device\": str(DEVICE),\n",
    "    }\n",
    "\n",
    "    if config['loss_type'] == 'combined':\n",
    "        config_dict['training']['t_solidus'] = config['t_solidus']\n",
    "        config_dict['training']['t_liquidus'] = config['t_liquidus']\n",
    "\n",
    "    with open(run_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    # Train with the CORRECT dataloaders for this model's sequence length\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train_microstructure(\n",
    "        model=model,\n",
    "        train_loader=model_train_loader,\n",
    "        val_loader=model_val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        epochs=EPOCHS,\n",
    "        run_dir=run_dir,\n",
    "        patience=PATIENCE,\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    with open(run_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    # Plot losses\n",
    "    plot_losses(history, str(run_dir / \"training_losses.png\"))\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': len(history['train_loss']),\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config_dict,\n",
    "        'history': history,\n",
    "    }, run_dir / \"checkpoints\" / \"final_model.pt\")\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  Epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "    return {\"status\": \"trained\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    results[config['name']] = train_model(config)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All models processed!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Generate predictions for timestep 23, slice 47 for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate prediction visualization for a model.\n",
    "    Also generates solidification mask visualization for combined loss models.\n",
    "    Skip if already exists.\n",
    "    \"\"\"\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    # For combined loss models, also check for solidification mask visualization\n",
    "    if config['loss_type'] == 'combined':\n",
    "        solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "        if pred_path.exists() and solid_mask_path.exists():\n",
    "            print(f\"Model {config['id']}: ✓ Prediction and solidification mask already exist\")\n",
    "            return\n",
    "    else:\n",
    "        if pred_path.exists():\n",
    "            print(f\"Model {config['id']}: ✓ Prediction already exists\")\n",
    "            return\n",
    "\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Model {config['id']}: ✗ No checkpoint found\")\n",
    "        return\n",
    "\n",
    "    print(f\"Model {config['id']}: Generating prediction...\")\n",
    "\n",
    "    try:\n",
    "        pred_micro, target_micro, future_temp, mask, metadata = load_model_and_predict(\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            timestep=PRED_TIMESTEP,\n",
    "            slice_index=PRED_SLICE,\n",
    "            sequence_length=config['seq_length'],\n",
    "            plane=PLANE,\n",
    "            device=str(DEVICE),\n",
    "        )\n",
    "\n",
    "        # Generate standard prediction visualization\n",
    "        save_prediction_visualization(\n",
    "            pred_micro=pred_micro,\n",
    "            target_micro=target_micro,\n",
    "            mask=mask,\n",
    "            save_path=str(pred_path),\n",
    "            title=f\"Model {config['id']}: {config['name']}\",\n",
    "        )\n",
    "        print(f\"  ✓ Saved prediction to {pred_path}\")\n",
    "\n",
    "        # Generate solidification mask visualization for combined loss models\n",
    "        if config['loss_type'] == 'combined':\n",
    "            # Create the loss function to get the weight map\n",
    "            loss_fn = CombinedLoss(\n",
    "                solidification_weight=0.7,\n",
    "                global_weight=0.3,\n",
    "                T_solidus=config['t_solidus'],\n",
    "                T_liquidus=config['t_liquidus'],\n",
    "                weight_type=\"gaussian\",\n",
    "                weight_scale=0.1,\n",
    "                base_weight=0.1,\n",
    "                return_components=True,\n",
    "            )\n",
    "\n",
    "            solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "            save_solidification_mask_visualization(\n",
    "                future_temp=future_temp,\n",
    "                pred_micro=pred_micro,\n",
    "                target_micro=target_micro,\n",
    "                mask=mask,\n",
    "                loss_fn=loss_fn,\n",
    "                save_path=str(solid_mask_path),\n",
    "                title=f\"Model {config['id']}: {config['name']}\",\n",
    "                timestep=metadata['timestep'],\n",
    "                slice_coord=metadata['slice_coord'],\n",
    "            )\n",
    "            print(f\"  ✓ Saved solidification mask to {solid_mask_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "print(f\"\\nGenerating predictions for timestep={PRED_TIMESTEP}, slice={PRED_SLICE}...\\n\")\n",
    "for config in MODEL_CONFIGS:\n",
    "    generate_prediction(config)\n",
    "\n",
    "print(\"\\nAll predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Training Losses\n",
    "\n",
    "Display training loss plots for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss Plots:\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    loss_plot_path = run_dir / \"training_losses.png\"\n",
    "\n",
    "    if loss_plot_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(loss_plot_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No training losses plot found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Predictions\n",
    "\n",
    "Display prediction visualizations for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if pred_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(pred_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No prediction found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Solidification Mask Visualizations\n",
    "\n",
    "Display solidification mask visualizations for models trained with combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Solidification Mask Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "print(\"(Only for models trained with combined loss)\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    # Skip non-combined loss models\n",
    "    if config['loss_type'] != 'combined':\n",
    "        continue\n",
    "\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if solid_mask_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(solid_mask_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No solidification mask found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare training losses across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all histories\n",
    "histories = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    history_path = OUTPUT_DIR / config['name'] / \"history.json\"\n",
    "    if history_path.exists():\n",
    "        with open(history_path, 'r') as f:\n",
    "            histories[config['name']] = json.load(f)\n",
    "\n",
    "if histories:\n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Training loss\n",
    "    for name, history in histories.items():\n",
    "        ax1.plot(history['train_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Validation loss\n",
    "    for name, history in histories.items():\n",
    "        ax2.plot(history['val_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Validation Loss', fontsize=12)\n",
    "    ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    comparison_path = OUTPUT_DIR / \"all_models_comparison.png\"\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nComparison plot saved to {comparison_path}\")\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'ID':<4} {'Name':<50} {'Final Train':<12} {'Final Val':<12} {'Epochs':<8}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        name = config['name']\n",
    "        if name in histories:\n",
    "            history = histories[name]\n",
    "            final_train = history['train_loss'][-1]\n",
    "            final_val = history['val_loss'][-1]\n",
    "            epochs = len(history['train_loss'])\n",
    "            print(f\"{config['id']:<4} {name:<50} {final_train:<12.6f} {final_val:<12.6f} {epochs:<8}\")\n",
    "        else:\n",
    "            print(f\"{config['id']:<4} {name:<50} {'N/A':<12} {'N/A':<12} {'N/A':<8}\")\n",
    "\n",
    "    print(\"=\"*100)\n",
    "else:\n",
    "    print(\"No training histories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All 10 models have been trained and evaluated!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
