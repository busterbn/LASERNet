{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Temperature Forecasting Benchmark Notebook\n",
    "\n",
    "Train several CNN-LSTM variations on the LASERNet temperature field, log each run just like the main MICROnet workflow, evaluate on validation + test sets, and finish with an autoregressive next-frame prediction demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add project root to the Python path so we can import training utilities\n",
    "project_root = Path(\"../\").resolve()\n",
    "if str(project_root) not in os.sys.path:\n",
    "    os.sys.path.append(str(project_root))\n",
    "\n",
    "from lasernet.dataset.calculate_temp import calculate_temp_stats_fast, get_default_temp_range\n",
    "from lasernet.micronet.dataset.fast_loading import FastSliceSequenceDataset\n",
    "from lasernet.model.CNN_LSTM import CNN_LSTM\n",
    "from train import evaluate_test, get_device, set_seed, train_tempnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "Define shared paths, random seeds, and helper utilities so that every experiment logs results like the main MICROnet notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "NOTEBOOK_RUN_ID = datetime.now().strftime(\"temperature_notebook_%Y%m%d_%H%M%S\")\n",
    "RUN_ROOT = (project_root / \"TempNet_output\")\n",
    "RUN_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "(RUN_ROOT / \"notes\").mkdir(exist_ok=True)\n",
    "print(f\"Run root: {RUN_ROOT}\")\n",
    "\n",
    "SPLIT_COUNTS = (10, 8, 6)\n",
    "train_ratio = SPLIT_COUNTS[0] / sum(SPLIT_COUNTS)\n",
    "val_ratio = SPLIT_COUNTS[1] / sum(SPLIT_COUNTS)\n",
    "test_ratio = SPLIT_COUNTS[2] / sum(SPLIT_COUNTS)\n",
    "DATA_PLANE = \"xz\"\n",
    "TARGET_OFFSET = 1\n",
    "NUM_WORKERS = 0\n",
    "NOTE = \"Notebook sweep of CNN-LSTM hyper-parameters\"\n",
    "\n",
    "try:\n",
    "    TEMP_RANGE = calculate_temp_stats_fast(\n",
    "        plane=DATA_PLANE,\n",
    "        split=\"train\",\n",
    "        sequence_length=3,\n",
    "        target_offset=TARGET_OFFSET,\n",
    "        train_ratio=train_ratio,\n",
    "        val_ratio=val_ratio,\n",
    "        test_ratio=test_ratio,\n",
    "        max_samples=128,\n",
    "    )\n",
    "except Exception as exc:  # Fallback if stats scan fails\n",
    "    print(f\"Falling back to default temperature range because: {exc}\")\n",
    "    TEMP_RANGE = get_default_temp_range()\n",
    "\n",
    "print(f\"Temperature normalization range: [{TEMP_RANGE[0]:.2f}, {TEMP_RANGE[1]:.2f}] K\")\n",
    "\n",
    "\n",
    "def create_run_dir(exp_name: str) -> Path:\n",
    "    \"\"\"Create a run directory that mirrors the CLI training structure.\"\"\"\n",
    "    run_dir = RUN_ROOT / exp_name\n",
    "    (run_dir / \"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"visualizations\").mkdir(exist_ok=True)\n",
    "    return run_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Fast Temperature Dataset Helpers\n",
    "\n",
    "Build cached `FastSliceSequenceDataset` objects for any sequence length so each experiment can reuse loaders without re-touching disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CACHE: Dict[int, Dict[str, FastSliceSequenceDataset]] = {}\n",
    "\n",
    "\n",
    "def build_datasets(sequence_length: int) -> Dict[str, FastSliceSequenceDataset]:\n",
    "    if sequence_length not in DATASET_CACHE:\n",
    "        print(f\"Instantiating datasets for sequence length {sequence_length}...\")\n",
    "        dataset_kwargs = dict(\n",
    "            plane=DATA_PLANE,\n",
    "            sequence_length=sequence_length,\n",
    "            target_offset=TARGET_OFFSET,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        DATASET_CACHE[sequence_length] = {\n",
    "            split: FastSliceSequenceDataset(split=split, **dataset_kwargs)\n",
    "            for split in (\"train\", \"val\", \"test\")\n",
    "        }\n",
    "    return DATASET_CACHE[sequence_length]\n",
    "\n",
    "\n",
    "def build_dataloaders(sequence_length: int, batch_size: int) -> Dict[str, DataLoader]:\n",
    "    datasets = build_datasets(sequence_length)\n",
    "    loaders = {\n",
    "        \"train\": DataLoader(\n",
    "            datasets[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            datasets[\"val\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            datasets[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=NUM_WORKERS,\n",
    "        ),\n",
    "    }\n",
    "    return loaders\n",
    "\n",
    "\n",
    "preview_datasets = build_datasets(sequence_length=3)\n",
    "preview_sample = preview_datasets[\"train\"][0]\n",
    "print(\"Sample shapes:\")\n",
    "print(f\"  Context: {preview_sample['context'].shape}\")\n",
    "print(f\"  Target:  {preview_sample['target'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Experiment Registry\n",
    "\n",
    "Set up a handful of CNN-LSTM variants (different sequence lengths, depths, and loss functions) so we can benchmark them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENTS: List[Dict] = [\n",
    "    # --- sequence length sweep, standard lr ---\n",
    "    {\n",
    "        \"name\": \"T01_seq2_lr1e-3\",\n",
    "        \"description\": \"CNN-LSTM with seq_len=2, 2 ConvLSTM layers, baseline channels.\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"T02_seq3_lr1e-3\",\n",
    "        \"description\": \"Same model with seq_len=3.\",\n",
    "        \"sequence_length\": 3,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"T03_seq4_lr1e-3\",\n",
    "        \"description\": \"Same model with seq_len=4 to test longer temporal context.\",\n",
    "        \"sequence_length\": 4,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "\n",
    "    # --- lr / stability tweaks on the best seq length (2 frames) ---\n",
    "    {\n",
    "        \"name\": \"T04_seq2_lr6e-4\",\n",
    "        \"description\": \"seq_len=2 with reduced learning rate for smoother training.\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "\n",
    "    # --- weight decay regularisation ---\n",
    "    {\n",
    "        \"name\": \"T05_seq2_lr1e-3_wd1e-5\",\n",
    "        \"description\": \"seq_len=2 with small weight decay for regularisation.\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "\n",
    "    # --- LSTM depth / capacity ---\n",
    "    {\n",
    "        \"name\": \"T06_seq2_lr1e-3_1layer\",\n",
    "        \"description\": \"seq_len=2 but only 1 ConvLSTM layer (capacity ablation).\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 1,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"T07_seq2_lr5e-4_2layers\",\n",
    "        \"description\": \"seq_len=2, 2 ConvLSTM layers, slightly lower lr.\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [16, 32, 64],\n",
    "        \"lstm_hidden\": 64,\n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 0.0,\n",
    "        \"visualize_every\": 0,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"T08_seq2_wider_channels\",\n",
    "        \"description\": \"seq_len=2 with wider encoder/decoder (24,48,96) and larger ConvLSTM state.\",\n",
    "        \"sequence_length\": 2,\n",
    "        \"epochs\": 200,\n",
    "        \"batch_size\": 16,\n",
    "        \"hidden_channels\": [24, 48, 96],  \n",
    "        \"lstm_hidden\": 96,                \n",
    "        \"lstm_layers\": 2,\n",
    "        \"learning_rate\": 5e-4,            \n",
    "        \"loss_fn\": \"mse\",\n",
    "        \"weight_decay\": 5e-5,             \n",
    "        \"visualize_every\": 0,\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(EXPERIMENTS)} experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "\n",
    "Helper functions to build models/losses, serialize configs, and execute each experiment while mirroring the CLI trainer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_loss_fn(exp_config: Dict) -> nn.Module:\n",
    "    loss_name = exp_config[\"loss_fn\"].lower()\n",
    "    if loss_name == \"smooth_l1\":\n",
    "        return nn.SmoothL1Loss(beta=exp_config.get(\"smooth_l1_beta\", 1.0))\n",
    "    if loss_name == \"mae\":\n",
    "        return nn.L1Loss()\n",
    "    if loss_name == \"mse\":\n",
    "        return nn.MSELoss()\n",
    "    raise ValueError(f\"Unknown loss function: {loss_name}\")\n",
    "\n",
    "\n",
    "def build_model_for_experiment(exp_config: Dict) -> CNN_LSTM:\n",
    "    return CNN_LSTM(\n",
    "        input_channels=1,\n",
    "        hidden_channels=exp_config[\"hidden_channels\"],\n",
    "        lstm_hidden=exp_config[\"lstm_hidden\"],\n",
    "        lstm_layers=exp_config[\"lstm_layers\"],\n",
    "        temp_range=TEMP_RANGE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "\n",
    "def dataset_metadata(sequence_length: int) -> Dict:\n",
    "    datasets = build_datasets(sequence_length)\n",
    "    return {\n",
    "        split: {\n",
    "            \"samples\": len(ds),\n",
    "            \"valid_sequences\": ds.num_valid_sequences,\n",
    "            \"num_slices\": len(ds.slice_coords),\n",
    "            \"timestep_span\": [int(min(ds.timestep_indices)), int(max(ds.timestep_indices))],\n",
    "        }\n",
    "        for split, ds in datasets.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def run_temperature_experiment(exp_config: Dict) -> Dict:\n",
    "    exp_cfg = deepcopy(exp_config)\n",
    "    seq_len = exp_cfg[\"sequence_length\"]\n",
    "    run_dir = create_run_dir(exp_cfg[\"name\"])\n",
    "\n",
    "    loaders = build_dataloaders(sequence_length=seq_len, batch_size=exp_cfg[\"batch_size\"])\n",
    "    meta = dataset_metadata(seq_len)\n",
    "\n",
    "    model = build_model_for_experiment(exp_cfg)\n",
    "    criterion = select_loss_fn(exp_cfg)\n",
    "    mae_fn = nn.L1Loss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=exp_cfg[\"learning_rate\"],\n",
    "        weight_decay=exp_cfg.get(\"weight_decay\", 0.0),\n",
    "    )\n",
    "\n",
    "    config_payload = {\n",
    "        \"note\": NOTE,\n",
    "        \"timestamp\": NOTEBOOK_RUN_ID,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"temp_range\": TEMP_RANGE,\n",
    "        \"experiment\": exp_cfg,\n",
    "        \"dataset\": meta,\n",
    "    }\n",
    "    config_path = run_dir / \"config.json\"\n",
    "    with config_path.open(\"w\") as fp:\n",
    "        json.dump(config_payload, fp, indent=2)\n",
    "\n",
    "    history = train_tempnet(\n",
    "        model=model,\n",
    "        train_loader=loaders[\"train\"],\n",
    "        val_loader=loaders[\"val\"],\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        mae_fn=mae_fn,\n",
    "        device=DEVICE,\n",
    "        epochs=exp_cfg[\"epochs\"],\n",
    "        run_dir=run_dir,\n",
    "        visualize_every=exp_cfg.get(\"visualize_every\", 0),\n",
    "        note=f\"{NOTE} | {exp_cfg['description']}\",\n",
    "    )\n",
    "\n",
    "    history_path = run_dir / \"history.json\"\n",
    "    with history_path.open(\"w\") as fp:\n",
    "        json.dump(history, fp, indent=2)\n",
    "\n",
    "    final_ckpt_path = run_dir / \"checkpoints\" / \"final_model.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": exp_cfg[\"epochs\"],\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"history\": history,\n",
    "            \"config\": config_payload,\n",
    "        },\n",
    "        final_ckpt_path,\n",
    "    )\n",
    "\n",
    "    # --- Load best model before testing ---\n",
    "    best_ckpt_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "    # --- Evaluate on test set using BEST model ---\n",
    "    print(f\"Testing best model from epoch {ckpt['epoch']}\")\n",
    "    test_results = evaluate_test(\n",
    "        model=model,\n",
    "        test_loader=loaders[\"test\"],\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        run_dir=run_dir,\n",
    "    )\n",
    "\n",
    "    test_results_path = run_dir / \"test_results.json\"\n",
    "    with test_results_path.open(\"w\") as fp:\n",
    "        json.dump(test_results, fp, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "    # ---- Compute “best” validation metrics ----\n",
    "    # For MSE loss, val_loss == val_mse\n",
    "    val_mse_best = float(min(history[\"val_loss\"]))\n",
    "    # best MAE over epochs\n",
    "    val_mae_best = float(min(history[\"val_mae\"]))\n",
    "    best_val_smoothed = float(min(history.get(\"val_loss_smoothed\", history[\"val_loss\"])))\n",
    "\n",
    "    # ---- Extract test metrics, being robust to key names ----\n",
    "    if \"test_mse\" in test_results:\n",
    "        test_mse = float(test_results[\"test_mse\"])\n",
    "        test_mae = float(test_results.get(\"test_mae\", float(\"nan\")))\n",
    "        test_loss_scalar = float(test_results.get(\"test_loss\", test_mse))\n",
    "    elif \"mse\" in test_results:\n",
    "        # like your older evaluate_test\n",
    "        test_mse = float(test_results[\"mse\"])\n",
    "        test_mae = float(test_results.get(\"mae\", float(\"nan\")))\n",
    "        test_loss_scalar = test_mse\n",
    "    else:\n",
    "        # fallback: only test_loss is available\n",
    "        test_mse = float(test_results[\"test_loss\"])\n",
    "        test_mae = float(test_results.get(\"test_mae\", float(\"nan\")))\n",
    "        test_loss_scalar = test_mse\n",
    "\n",
    "    summary = {\n",
    "        \"name\": exp_cfg[\"name\"],\n",
    "        \"description\": exp_cfg[\"description\"],\n",
    "        \"sequence_length\": seq_len,\n",
    "        \"epochs\": exp_cfg[\"epochs\"],\n",
    "        \"batch_size\": exp_cfg[\"batch_size\"],\n",
    "        \"loss_fn\": exp_cfg[\"loss_fn\"],\n",
    "        \"run_dir\": str(run_dir),\n",
    "        \"config_path\": str(config_path),\n",
    "        \"history_path\": str(history_path),\n",
    "        \"test_results_path\": str(test_results_path),\n",
    "\n",
    "        # training curves\n",
    "        \"train_loss_last\": float(history[\"train_loss\"][-1]),\n",
    "        \"val_loss_last\": float(history[\"val_loss\"][-1]),\n",
    "        \"val_loss_best\": val_mse_best,\n",
    "        \"val_loss_smoothed_best\": best_val_smoothed,\n",
    "        \"train_mae_last\": float(history[\"train_mae\"][-1]),\n",
    "        \"val_mae_last\": float(history[\"val_mae\"][-1]),\n",
    "\n",
    "        # explicit metrics for the leaderboard\n",
    "        \"val_mse\": val_mse_best,\n",
    "        \"val_mae\": val_mae_best,\n",
    "        \"test_mse\": test_mse,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"test_loss\": test_loss_scalar,\n",
    "\n",
    "        \"num_test_samples\": int(test_results[\"num_samples\"]),\n",
    "        \"best_checkpoint\": str(run_dir / \"checkpoints\" / \"best_model.pt\"),\n",
    "        \"exp_config\": exp_cfg,\n",
    "    }\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Train + Evaluate All Experiments\n",
    "\n",
    "Loop through the registry, train each configuration, store artifacts, and keep a lightweight summary for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_summaries: List[Dict] = []\n",
    "\n",
    "for exp in EXPERIMENTS:\n",
    "    print(\"=\" * 90)\n",
    "    print(f\"Starting experiment: {exp['name']} ({exp['description']})\")\n",
    "    summary = run_temperature_experiment(exp)\n",
    "    experiment_summaries.append(summary)\n",
    "    print(\n",
    "        f\"Finished {exp['name']} | best val (smoothed): {summary['val_loss_smoothed_best']:.4f} | \"\n",
    "        f\"test: {summary['test_loss']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(f\"Completed {len(experiment_summaries)} experiments. Artifacts saved under {RUN_ROOT}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Experiment Leaderboard\n",
    "\n",
    "Summarize validation/test metrics and surface the best checkpoint for downstream inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not experiment_summaries:\n",
    "    raise RuntimeError(\"No experiments were executed. Please run the previous cell first.\")\n",
    "\n",
    "leaderboard_columns = [\n",
    "    \"name\",\n",
    "    \"description\",\n",
    "    \"sequence_length\",\n",
    "    \"epochs\",\n",
    "    \"batch_size\",\n",
    "    \"loss_fn\",\n",
    "    \"val_mse\",\n",
    "    \"val_mae\",\n",
    "    \"test_mse\",\n",
    "    \"test_mae\",\n",
    "    \"val_loss_smoothed_best\",\n",
    "    \"val_loss_best\",\n",
    "    \"run_dir\",\n",
    "]\n",
    "\n",
    "leaderboard_df = pd.DataFrame(experiment_summaries).copy()\n",
    "for column in leaderboard_columns:\n",
    "    if column not in leaderboard_df.columns:\n",
    "        leaderboard_df[column] = np.nan\n",
    "\n",
    "leaderboard_df = (\n",
    "    leaderboard_df[leaderboard_columns]\n",
    "    .sort_values(\"val_mse\", na_position=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "leaderboard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Best Run Snapshot\n",
    "\n",
    "Pick the checkpoint with the lowest smoothed validation loss so we can drive downstream predictions and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EXPERIMENT = min(\n",
    "    experiment_summaries,\n",
    "    key=lambda summary: summary[\"val_loss_smoothed_best\"],\n",
    ")\n",
    "\n",
    "val_mse = float(BEST_EXPERIMENT.get(\"val_mse\", float(\"nan\")))\n",
    "val_mae = float(BEST_EXPERIMENT.get(\"val_mae\", float(\"nan\")))\n",
    "test_mse = float(BEST_EXPERIMENT.get(\"test_mse\", float(\"nan\")))\n",
    "test_mae = float(BEST_EXPERIMENT.get(\"test_mae\", float(\"nan\")))\n",
    "\n",
    "print(\n",
    "    \"Best experiment: {name} | val_smoothed={val_smoothed:.4f} | val_mse={val_mse:.4f} | \"\n",
    "    \"val_mae={val_mae:.4f} | test_mse={test_mse:.4f} | test_mae={test_mae:.4f}\".format(\n",
    "        name=BEST_EXPERIMENT[\"name\"],\n",
    "        val_smoothed=BEST_EXPERIMENT[\"val_loss_smoothed_best\"],\n",
    "        val_mse=val_mse,\n",
    "        val_mae=val_mae,\n",
    "        test_mse=test_mse,\n",
    "        test_mae=test_mae,\n",
    "    )\n",
    ")\n",
    "print(f\"Artifacts: {BEST_EXPERIMENT['run_dir']}\")\n",
    "\n",
    "best_run_dir = Path(BEST_EXPERIMENT[\"run_dir\"])\n",
    "best_config = BEST_EXPERIMENT[\"exp_config\"]\n",
    "best_checkpoint_path = Path(BEST_EXPERIMENT[\"best_checkpoint\"])\n",
    "print(f\"Using checkpoint: {best_checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Autoregressive Next-Frame Prediction\n",
    "\n",
    "Use the best checkpoint to roll out several future frames autoregressively on a held-out slice to inspect qualitative performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_seq_len = best_config[\"sequence_length\"]\n",
    "num_initial_frames = best_seq_len\n",
    "num_predictions = 3\n",
    "\n",
    "datasets_best = build_datasets(best_seq_len)\n",
    "test_dataset = datasets_best[\"test\"]\n",
    "slice_idx = len(test_dataset.slice_coords) // 2\n",
    "slice_coord = test_dataset.slice_coords[slice_idx]\n",
    "\n",
    "# Ensure we have enough timesteps for the rollout\n",
    "max_frames_available = len(test_dataset.timestep_indices) - 1\n",
    "max_total_frames = min(num_initial_frames + num_predictions, max_frames_available)\n",
    "if max_total_frames <= num_initial_frames:\n",
    "    raise RuntimeError(\"Not enough timesteps to perform autoregressive rollout.\")\n",
    "\n",
    "# Recompute predictions count in case we had to clamp\n",
    "num_predictions = max_total_frames - num_initial_frames\n",
    "\n",
    "# Gather ground-truth frames directly from the cached tensors\n",
    "plane_tensor = test_dataset._extract_plane_slice(test_dataset.temp_data, slice_idx)\n",
    "ground_truth_frames = []\n",
    "for offset in range(max_total_frames):\n",
    "    timestep_idx = test_dataset.timestep_indices[1 + offset]\n",
    "    ground_truth_frames.append(plane_tensor[timestep_idx].unsqueeze(0))\n",
    "\n",
    "ground_truth = torch.stack(ground_truth_frames, dim=0)\n",
    "print(f\"Loaded ground truth sequence with shape {ground_truth.shape} for slice {slice_coord:.4f}\")\n",
    "\n",
    "# Load best checkpoint\n",
    "best_model = build_model_for_experiment(best_config)\n",
    "ckpt = torch.load(best_checkpoint_path, map_location=DEVICE)\n",
    "best_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "best_model.eval()\n",
    "\n",
    "predictions = [ground_truth[i] for i in range(num_initial_frames)]\n",
    "frame_metrics = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(num_predictions):\n",
    "        window = torch.stack(predictions[-num_initial_frames:], dim=0)  # [seq, 1, H, W]\n",
    "        pred = best_model(window.unsqueeze(0).to(DEVICE)).cpu()\n",
    "        predictions.append(pred.squeeze(0))\n",
    "\n",
    "        frame_idx = num_initial_frames + step\n",
    "        gt_frame = ground_truth[frame_idx]\n",
    "        mse = float(((pred.squeeze(0) - gt_frame) ** 2).mean())\n",
    "        mae = float((pred.squeeze(0) - gt_frame).abs().mean())\n",
    "        frame_metrics.append({\"frame\": frame_idx, \"mse\": mse, \"mae\": mae})\n",
    "        print(f\"Frame {frame_idx}: MSE={mse:.2f} | MAE={mae:.2f}\")\n",
    "\n",
    "predictions_tensor = torch.stack(predictions, dim=0)\n",
    "print(f\"Prediction tensor shape: {predictions_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Visual Comparison\n",
    "\n",
    "Plot ground truth, predictions, and absolute error for each frame used in the rollout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_frames = predictions_tensor.shape[0]\n",
    "fig, axes = plt.subplots(3, total_frames, figsize=(2.4 * total_frames, 7.5))\n",
    "\n",
    "vmin = float(ground_truth.min())\n",
    "vmax = float(ground_truth.max())\n",
    "error_vmax = 150.0\n",
    "\n",
    "for idx in range(total_frames):\n",
    "    gt = ground_truth[idx, 0].numpy()\n",
    "    pred = predictions_tensor[idx, 0].numpy()\n",
    "\n",
    "    axes[0, idx].imshow(gt, cmap=\"hot\", vmin=vmin, vmax=vmax, origin=\"lower\")\n",
    "    axes[0, idx].set_title(f\"GT frame {idx}\")\n",
    "    axes[0, idx].axis(\"off\")\n",
    "\n",
    "    axes[1, idx].imshow(pred, cmap=\"hot\", vmin=vmin, vmax=vmax, origin=\"lower\")\n",
    "    title = \"context\" if idx < num_initial_frames else \"pred\"\n",
    "    axes[1, idx].set_title(f\"{title} {idx}\")\n",
    "    axes[1, idx].axis(\"off\")\n",
    "\n",
    "    err = np.abs(pred - gt)\n",
    "    axes[2, idx].imshow(err, cmap=\"viridis\", vmin=0, vmax=error_vmax, origin=\"lower\")\n",
    "    axes[2, idx].set_title(f\"MAE {err.mean():.1f} K\")\n",
    "    axes[2, idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Error Evolution\n",
    "\n",
    "Track MSE/MAE per frame to quantify how much the autoregressive rollout drifts as we step forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_series = [0.0] * num_initial_frames + [m[\"mae\"] for m in frame_metrics]\n",
    "mse_series = [0.0] * num_initial_frames + [m[\"mse\"] for m in frame_metrics]\n",
    "frames = list(range(len(mae_series)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(11, 4))\n",
    "ax1.plot(frames, mse_series, marker=\"o\")\n",
    "ax1.axvline(num_initial_frames - 0.5, color=\"red\", linestyle=\"--\", label=\"predictions start\")\n",
    "ax1.set_title(\"MSE per frame\")\n",
    "ax1.set_xlabel(\"Frame\")\n",
    "ax1.set_ylabel(\"MSE (K²)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(frames, mae_series, marker=\"o\", color=\"orange\")\n",
    "ax2.axvline(num_initial_frames - 0.5, color=\"red\", linestyle=\"--\", label=\"predictions start\")\n",
    "ax2.set_title(\"MAE per frame\")\n",
    "ax2.set_xlabel(\"Frame\")\n",
    "ax2.set_ylabel(\"MAE (K)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "frame_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
