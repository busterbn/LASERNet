{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICROnet Training - Comprehensive Model Comparison\n",
    "\n",
    "This notebook trains 16 different microstructure prediction models using U-Net architecture with skip connections:\n",
    "\n",
    "## CNN-LSTM Models (8 models):\n",
    "### MSE Loss:\n",
    "1. seq=2, MSE loss\n",
    "2. seq=3, MSE loss\n",
    "3. seq=4, MSE loss\n",
    "\n",
    "### Combined Loss (seq=3):\n",
    "4. T_solidus=1560, T_liquidus=1620, weights=70/30\n",
    "5. T_solidus=1530, T_liquidus=1650, weights=70/30\n",
    "6. T_solidus=1500, T_liquidus=1680, weights=70/30\n",
    "7. T_solidus=1560, T_liquidus=1620, weights=50/50\n",
    "8. T_solidus=1560, T_liquidus=1620, weights=30/70\n",
    "\n",
    "## PredRNN Models (8 models):\n",
    "### MSE Loss:\n",
    "9. seq=2, MSE loss\n",
    "10. seq=3, MSE loss\n",
    "11. seq=4, MSE loss\n",
    "\n",
    "### Combined Loss (seq=2):\n",
    "12. T_solidus=1560, T_liquidus=1620, weights=70/30\n",
    "13. T_solidus=1530, T_liquidus=1650, weights=70/30\n",
    "14. T_solidus=1500, T_liquidus=1680, weights=70/30\n",
    "15. T_solidus=1560, T_liquidus=1620, weights=50/50\n",
    "16. T_solidus=1560, T_liquidus=1620, weights=30/70\n",
    "\n",
    "All models use U-Net architecture with skip connections for improved gradient flow and feature reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path so we can import lasernet\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from lasernet.micronet.train.trainer import (\n",
    "    get_device,\n",
    "    train_microstructure,\n",
    "    load_model_and_predict,\n",
    "    save_prediction_visualization,\n",
    "    save_solidification_mask_visualization,\n",
    ")\n",
    "from lasernet.micronet.model.MicrostructureCNN_LSTM import MicrostructureCNN_LSTM\n",
    "from lasernet.micronet.model.MicrostructurePredRNN import MicrostructurePredRNN\n",
    "from lasernet.micronet.model.losses import CombinedLoss\n",
    "from lasernet.micronet.utils import plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds for Reproducibility\n",
    "\n",
    "Set all random seeds to ensure reproducible results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set Python random seed\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set NumPy random seed\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set PyTorch random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU setups\n",
    "\n",
    "# Configure PyTorch to use deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment variable for additional determinism\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED} for reproducibility\")\n",
    "print(\"  ✓ Python random\")\n",
    "print(\"  ✓ NumPy\")\n",
    "print(\"  ✓ PyTorch (CPU and CUDA)\")\n",
    "print(\"  ✓ cuDNN deterministic mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in project root\n",
    "project_root = Path.cwd().parent\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    load_dotenv(dotenv_path=env_file, override=True)\n",
    "    print(f\"Loaded .env from: {env_file}\")\n",
    "else:\n",
    "    print(f\"Warning: .env file not found at {env_file}\")\n",
    "\n",
    "# Verify BLACKHOLE is set\n",
    "blackhole_path = os.environ.get(\"BLACKHOLE\")\n",
    "if blackhole_path:\n",
    "    print(f\"BLACKHOLE environment variable: {blackhole_path}\")\n",
    "else:\n",
    "    raise ValueError(\"BLACKHOLE environment variable not set. Please create a .env file with BLACKHOLE=/path/to/data\")\n",
    "\n",
    "# Global configuration\n",
    "DEVICE = get_device()\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 15\n",
    "OUTPUT_DIR = Path(\"MICROnet_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Prediction settings\n",
    "PRED_TIMESTEP = 23\n",
    "PRED_SLICE = 47\n",
    "PLANE = \"xz\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Prediction settings: timestep={PRED_TIMESTEP}, slice={PRED_SLICE}, plane={PRED_TIMESTEP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load datasets using fast loading from preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (same approach as notebook.ipynb)\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "from pathlib import Path as PathLib\n",
    "from lasernet.micronet.dataset import MicrostructureSequenceDataset\n",
    "from lasernet.micronet.dataset.fast_loading import FastMicrostructureSequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "SEQ_LENGTH = 4  # Default sequence length for dataset creation\n",
    "SPLIT_RATIO = \"12,6,6\"\n",
    "\n",
    "# Parse split ratios\n",
    "split_ratios = list(map(int, SPLIT_RATIO.split(\",\")))\n",
    "train_ratio = split_ratios[0] / sum(split_ratios)\n",
    "val_ratio = split_ratios[1] / sum(split_ratios)\n",
    "test_ratio = split_ratios[2] / sum(split_ratios)\n",
    "\n",
    "# Check if preprocessed files are available for fast loading\n",
    "blackhole = os.environ.get(\"BLACKHOLE\")\n",
    "if not blackhole:\n",
    "    raise ValueError(\"BLACKHOLE environment variable not set. Please set it in the makefile or shell.\")\n",
    "\n",
    "print(f\"BLACKHOLE directory: {blackhole}\")\n",
    "\n",
    "processed_dir = PathLib(blackhole) / \"processed\" / \"data\"\n",
    "required_files = [\"coordinates.pt\", \"microstructure.pt\", \"temperature.pt\"]\n",
    "fast_loading_available = all((processed_dir / f).exists() for f in required_files)\n",
    "\n",
    "if fast_loading_available:\n",
    "    print(\"✓ Preprocessed files found - using fast loading\")\n",
    "    DatasetClass = FastMicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "else:\n",
    "    print(\"⚠ Preprocessed files not found - using standard loading\")\n",
    "    DatasetClass = MicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"preload\": True,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "\n",
    "train_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"train\"})\n",
    "val_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"val\"})\n",
    "test_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"test\"})\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples:   {len(val_dataset)}\")\n",
    "print(f\"  Test samples:  {len(test_dataset)}\")\n",
    "\n",
    "# Show sample dimensions\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample dimensions:\")\n",
    "print(f\"  Context temp:  {sample['context_temp'].shape}\")\n",
    "print(f\"  Context micro: {sample['context_micro'].shape}\")\n",
    "print(f\"  Future temp:   {sample['future_temp'].shape}\")\n",
    "print(f\"  Target micro:  {sample['target_micro'].shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "Define all 10 model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all model configurations\n",
    "MODEL_CONFIGS = [\n",
    "    # CNN-LSTM models with MSE loss\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"01_MICROnet_cnn_lstm_seq2_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"02_MICROnet_cnn_lstm_seq3_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"03_MICROnet_cnn_lstm_seq4_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # CNN-LSTM models with Combined loss\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"04_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"name\": \"05_MICROnet_cnn_lstm_seq3_CombLoss_T1530-1650_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"name\": \"06_MICROnet_cnn_lstm_seq3_CombLoss_T1500-1680_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"name\": \"07_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s50_g50\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.5,\n",
    "        \"global_weight\": 0.5,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"name\": \"08_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s30_g70\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.3,\n",
    "        \"global_weight\": 0.7,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # PredRNN models with MSE loss\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"name\": \"09_MICROnet_predrnn_seq2_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"name\": \"10_MICROnet_predrnn_seq3_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"name\": \"11_MICROnet_predrnn_seq4_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # PredRNN models with Combined loss\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"name\": \"12_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"name\": \"13_MICROnet_predrnn_seq2_CombLoss_T1530-1650_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"name\": \"14_MICROnet_predrnn_seq2_CombLoss_T1500-1680_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"name\": \"15_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s50_g50\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.5,\n",
    "        \"global_weight\": 0.5,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"name\": \"16_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s30_g70\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.3,\n",
    "        \"global_weight\": 0.7,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(MODEL_CONFIGS)} models for training:\")\n",
    "print(f\"\\n{'ID':<4} {'Model':<10} {'Seq':<5} {'Loss':<40}\")\n",
    "print(\"-\" * 65)\n",
    "for cfg in MODEL_CONFIGS:\n",
    "    skip_str = \" (U-Net)\" if cfg.get('use_skip_connections', False) else \"\"\n",
    "    loss_str = f\"MSE\" if cfg['loss_type'] == 'mse' else f\"Combined T{cfg['t_solidus']:.0f}-{cfg['t_liquidus']:.0f}\"\n",
    "    print(f\"{cfg['id']:<4} {cfg['model_type']:<10} {cfg['seq_length']:<5} {loss_str:<40}{skip_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train each model. Skip if already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Train a single model based on configuration.\n",
    "    Skip if already trained.\n",
    "    \"\"\"\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "    # Check if already trained\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Model {config['id']}: {config['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"✓ Model already trained. Skipping training.\")\n",
    "\n",
    "        # Load history if available\n",
    "        history_path = run_dir / \"history.json\"\n",
    "        if history_path.exists():\n",
    "            with open(history_path, 'r') as f:\n",
    "                history = json.load(f)\n",
    "        else:\n",
    "            history = None\n",
    "\n",
    "        return {\"status\": \"skipped\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "    # Create directories\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model {config['id']}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Model type: {config['model_type']}\")\n",
    "    print(f\"  Sequence length: {config['seq_length']}\")\n",
    "    print(f\"  Loss type: {config['loss_type']}\")\n",
    "    print(f\"  Skip connections: {config.get('use_skip_connections', False)}\")\n",
    "    if config['loss_type'] == 'combined':\n",
    "        print(f\"  T_solidus: {config['t_solidus']} K\")\n",
    "        print(f\"  T_liquidus: {config['t_liquidus']} K\")\n",
    "        print(f\"  Solidification weight: {config.get('solidification_weight', 0.7)}\")\n",
    "        print(f\"  Global weight: {config.get('global_weight', 0.3)}\")\n",
    "\n",
    "    # FIX: Create dataloaders with the CORRECT sequence length for THIS model\n",
    "    print(f\"\\n  Creating datasets with sequence length {config['seq_length']}...\")\n",
    "    if fast_loading_available:\n",
    "        model_train_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "    else:\n",
    "        model_train_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "\n",
    "    model_train_loader = DataLoader(model_train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    model_val_loader = DataLoader(model_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"  ✓ Datasets created: train={len(model_train_dataset)}, val={len(model_val_dataset)}\")\n",
    "\n",
    "    # Create model with skip connections option\n",
    "    use_skip_connections = config.get('use_skip_connections', False)\n",
    "\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        model = MicrostructureCNN_LSTM(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "    else:  # predrnn\n",
    "        model = MicrostructurePredRNN(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    param_count = model.count_parameters()\n",
    "    print(f\"  Parameters: {param_count:,}\")\n",
    "\n",
    "    # Create loss function\n",
    "    if config['loss_type'] == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:  # combined\n",
    "        criterion = CombinedLoss(\n",
    "            solidification_weight=config.get('solidification_weight', 0.7),\n",
    "            global_weight=config.get('global_weight', 0.3),\n",
    "            T_solidus=config['t_solidus'],\n",
    "            T_liquidus=config['t_liquidus'],\n",
    "            weight_type=\"gaussian\",\n",
    "            weight_scale=0.1,\n",
    "            base_weight=0.1,\n",
    "            return_components=True,  # Enable component tracking for visualization\n",
    "        )\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        \"model\": {\n",
    "            \"name\": config['model_type'],\n",
    "            \"parameters\": param_count,\n",
    "            \"sequence_length\": config['seq_length'],\n",
    "            \"use_skip_connections\": use_skip_connections,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"loss_type\": config['loss_type'],\n",
    "        },\n",
    "        \"device\": str(DEVICE),\n",
    "    }\n",
    "\n",
    "    if config['loss_type'] == 'combined':\n",
    "        config_dict['training']['t_solidus'] = config['t_solidus']\n",
    "        config_dict['training']['t_liquidus'] = config['t_liquidus']\n",
    "        config_dict['training']['solidification_weight'] = config.get('solidification_weight', 0.7)\n",
    "        config_dict['training']['global_weight'] = config.get('global_weight', 0.3)\n",
    "\n",
    "    with open(run_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    # Train with the CORRECT dataloaders for this model's sequence length\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train_microstructure(\n",
    "        model=model,\n",
    "        train_loader=model_train_loader,\n",
    "        val_loader=model_val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        epochs=EPOCHS,\n",
    "        run_dir=run_dir,\n",
    "        patience=PATIENCE,\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    with open(run_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    # Plot losses\n",
    "    plot_losses(history, str(run_dir / \"training_losses.png\"))\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': len(history['train_loss']),\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config_dict,\n",
    "        'history': history,\n",
    "    }, run_dir / \"checkpoints\" / \"final_model.pt\")\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  Epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "    return {\"status\": \"trained\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    results[config['name']] = train_model(config)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All models processed!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions\n",
    "\n",
    "Generate predictions for timestep 23, slice 47 for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate prediction visualization for a model.\n",
    "    Also generates solidification mask visualization for combined loss models.\n",
    "    Skip if already exists.\n",
    "    \"\"\"\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    # Check if prediction already exists\n",
    "    if pred_path.exists():\n",
    "        # For combined loss models, also check solidification mask\n",
    "        if config['loss_type'] == 'combined':\n",
    "            solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "            if solid_mask_path.exists():\n",
    "                print(f\"Model {config['id']}: ✓ Predictions already exist. Skipping.\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"Model {config['id']}: ✓ Prediction already exists. Skipping.\")\n",
    "            return\n",
    "\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Model {config['id']}: ✗ No checkpoint found\")\n",
    "        return\n",
    "\n",
    "    print(f\"Model {config['id']}: Generating prediction...\")\n",
    "\n",
    "    try:\n",
    "        pred_micro, target_micro, future_temp, mask, metadata = load_model_and_predict(\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            timestep=PRED_TIMESTEP,\n",
    "            slice_index=PRED_SLICE,\n",
    "            sequence_length=config['seq_length'],\n",
    "            plane=PLANE,\n",
    "            device=str(DEVICE),\n",
    "        )\n",
    "\n",
    "        # Create loss function for visualization\n",
    "        if config['loss_type'] == 'mse':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        else:  # combined\n",
    "            loss_fn = CombinedLoss(\n",
    "                solidification_weight=config.get('solidification_weight', 0.7),\n",
    "                global_weight=config.get('global_weight', 0.3),\n",
    "                T_solidus=config['t_solidus'],\n",
    "                T_liquidus=config['t_liquidus'],\n",
    "                weight_type=\"gaussian\",\n",
    "                weight_scale=0.1,\n",
    "                base_weight=0.1,\n",
    "                return_components=True,\n",
    "            )\n",
    "\n",
    "        # Generate standard prediction visualization\n",
    "        save_prediction_visualization(\n",
    "            pred_micro=pred_micro,\n",
    "            target_micro=target_micro,\n",
    "            mask=mask,\n",
    "            save_path=str(pred_path),\n",
    "            title=f\"Model {config['id']}: {config['name']}\",\n",
    "            future_temp=future_temp,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "        print(f\"  ✓ Saved prediction to {pred_path}\")\n",
    "\n",
    "        # Generate solidification mask visualization for combined loss models\n",
    "        if config['loss_type'] == 'combined':\n",
    "            solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "            save_solidification_mask_visualization(\n",
    "                future_temp=future_temp,\n",
    "                pred_micro=pred_micro,\n",
    "                target_micro=target_micro,\n",
    "                mask=mask,\n",
    "                loss_fn=loss_fn,\n",
    "                save_path=str(solid_mask_path),\n",
    "                title=f\"Model {config['id']}: {config['name']}\",\n",
    "                timestep=metadata['timestep'],\n",
    "                slice_coord=metadata['slice_coord'],\n",
    "            )\n",
    "            print(f\"  ✓ Saved solidification mask to {solid_mask_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "print(f\"\\nGenerating predictions for timestep={PRED_TIMESTEP}, slice={PRED_SLICE}...\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    generate_prediction(config)\n",
    "\n",
    "print(\"\\nAll predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Training Losses\n",
    "\n",
    "Display training loss plots for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss Plots:\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    loss_plot_path = run_dir / \"training_losses.png\"\n",
    "\n",
    "    if loss_plot_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(loss_plot_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No training losses plot found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Predictions\n",
    "\n",
    "Display prediction visualizations for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if pred_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(pred_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No prediction found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Solidification Mask Visualizations\n",
    "\n",
    "Display solidification mask visualizations for models trained with combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Solidification Mask Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "print(\"(Only for models trained with combined loss)\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    # Skip non-combined loss models\n",
    "    if config['loss_type'] != 'combined':\n",
    "        continue\n",
    "\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if solid_mask_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(solid_mask_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No solidification mask found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare training losses across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all histories\n",
    "histories = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    history_path = OUTPUT_DIR / config['name'] / \"history.json\"\n",
    "    if history_path.exists():\n",
    "        with open(history_path, 'r') as f:\n",
    "            histories[config['name']] = json.load(f)\n",
    "\n",
    "if histories:\n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Training loss\n",
    "    for name, history in histories.items():\n",
    "        ax1.plot(history['train_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Validation loss\n",
    "    for name, history in histories.items():\n",
    "        ax2.plot(history['val_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Validation Loss', fontsize=12)\n",
    "    ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    comparison_path = OUTPUT_DIR / \"all_models_comparison.png\"\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nComparison plot saved to {comparison_path}\")\n",
    "\n",
    "    # Calculate solidification loss for MSE models on validation set\n",
    "    print(\"\\nCalculating solidification loss for MSE models on validation set...\")\n",
    "\n",
    "    # Create a CombinedLoss function for solidification region evaluation (T_solidus=1560, T_liquidus=1620)\n",
    "    solidification_loss_fn = CombinedLoss(\n",
    "        solidification_weight=1.0,\n",
    "        global_weight=0.0,\n",
    "        T_solidus=1560.0,\n",
    "        T_liquidus=1620.0,\n",
    "        weight_type=\"gaussian\",\n",
    "        weight_scale=0.1,\n",
    "        base_weight=0.1,\n",
    "        return_components=True,\n",
    "    )\n",
    "\n",
    "    mse_solidification_losses = {}\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        if config['loss_type'] == 'mse':\n",
    "            checkpoint_path = OUTPUT_DIR / config['name'] / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "            if checkpoint_path.exists():\n",
    "                print(f\"  Model {config['id']}: Computing solidification loss...\")\n",
    "\n",
    "                # Load model\n",
    "                checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "                if config['model_type'] == 'cnn_lstm':\n",
    "                    model = MicrostructureCNN_LSTM(\n",
    "                        input_channels=10,\n",
    "                        future_channels=1,\n",
    "                        output_channels=9,\n",
    "                        use_skip_connections=config.get('use_skip_connections', False),\n",
    "                    ).to(DEVICE)\n",
    "                else:  # predrnn\n",
    "                    model = MicrostructurePredRNN(\n",
    "                        input_channels=10,\n",
    "                        future_channels=1,\n",
    "                        output_channels=9,\n",
    "                        use_skip_connections=config.get('use_skip_connections', False),\n",
    "                    ).to(DEVICE)\n",
    "\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model.eval()\n",
    "\n",
    "                # Create validation dataset with correct sequence length\n",
    "                if fast_loading_available:\n",
    "                    eval_dataset = FastMicrostructureSequenceDataset(\n",
    "                        plane=PLANE,\n",
    "                        split=\"val\",\n",
    "                        sequence_length=config['seq_length'],\n",
    "                        target_offset=1,\n",
    "                        train_ratio=train_ratio,\n",
    "                        val_ratio=val_ratio,\n",
    "                        test_ratio=test_ratio,\n",
    "                    )\n",
    "                else:\n",
    "                    eval_dataset = MicrostructureSequenceDataset(\n",
    "                        plane=PLANE,\n",
    "                        split=\"val\",\n",
    "                        sequence_length=config['seq_length'],\n",
    "                        target_offset=1,\n",
    "                        preload=True,\n",
    "                        train_ratio=train_ratio,\n",
    "                        val_ratio=val_ratio,\n",
    "                        test_ratio=test_ratio,\n",
    "                    )\n",
    "\n",
    "                eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                total_solid_loss = 0.0\n",
    "                num_batches = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch in eval_loader:\n",
    "                        context_temp = batch['context_temp'].to(DEVICE)\n",
    "                        context_micro = batch['context_micro'].to(DEVICE)\n",
    "                        future_temp = batch['future_temp'].to(DEVICE)\n",
    "                        target_micro = batch['target_micro'].to(DEVICE)\n",
    "                        mask = batch['target_mask'].to(DEVICE)\n",
    "\n",
    "                        # Concatenate context temperature and microstructure\n",
    "                        context = torch.cat([context_temp, context_micro], dim=2)\n",
    "\n",
    "                        # Forward pass\n",
    "                        pred_micro = model(context, future_temp)\n",
    "\n",
    "                        # Calculate solidification loss\n",
    "                        total_loss, solid_loss, global_loss = solidification_loss_fn(pred_micro, target_micro, future_temp, mask)\n",
    "\n",
    "                        total_solid_loss += solid_loss.item()\n",
    "                        num_batches += 1\n",
    "\n",
    "                avg_solid_loss = total_solid_loss / num_batches\n",
    "                mse_solidification_losses[config['name']] = avg_solid_loss\n",
    "                print(f\"    ✓ Solidification loss: {avg_solid_loss:.6f}\")\n",
    "\n",
    "    # Calculate test set metrics for all models\n",
    "    print(\"\\nCalculating test set metrics for all models...\")\n",
    "\n",
    "    test_mse_losses = {}\n",
    "    test_solid_losses = {}\n",
    "\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        checkpoint_path = OUTPUT_DIR / config['name'] / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "        if checkpoint_path.exists():\n",
    "            print(f\"  Model {config['id']}: Computing test set metrics...\")\n",
    "\n",
    "            # Load model\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "            if config['model_type'] == 'cnn_lstm':\n",
    "                model = MicrostructureCNN_LSTM(\n",
    "                    input_channels=10,\n",
    "                    future_channels=1,\n",
    "                    output_channels=9,\n",
    "                    use_skip_connections=config.get('use_skip_connections', False),\n",
    "                ).to(DEVICE)\n",
    "            else:  # predrnn\n",
    "                model = MicrostructurePredRNN(\n",
    "                    input_channels=10,\n",
    "                    future_channels=1,\n",
    "                    output_channels=9,\n",
    "                    use_skip_connections=config.get('use_skip_connections', False),\n",
    "                ).to(DEVICE)\n",
    "\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "\n",
    "            # Create test dataset with correct sequence length\n",
    "            if fast_loading_available:\n",
    "                test_dataset = FastMicrostructureSequenceDataset(\n",
    "                    plane=PLANE,\n",
    "                    split=\"test\",\n",
    "                    sequence_length=config['seq_length'],\n",
    "                    target_offset=1,\n",
    "                    train_ratio=train_ratio,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio,\n",
    "                )\n",
    "            else:\n",
    "                test_dataset = MicrostructureSequenceDataset(\n",
    "                    plane=PLANE,\n",
    "                    split=\"test\",\n",
    "                    sequence_length=config['seq_length'],\n",
    "                    target_offset=1,\n",
    "                    preload=True,\n",
    "                    train_ratio=train_ratio,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio,\n",
    "                )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            total_mse_loss = 0.0\n",
    "            total_solid_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    context_temp = batch['context_temp'].to(DEVICE)\n",
    "                    context_micro = batch['context_micro'].to(DEVICE)\n",
    "                    future_temp = batch['future_temp'].to(DEVICE)\n",
    "                    target_micro = batch['target_micro'].to(DEVICE)\n",
    "                    mask = batch['target_mask'].to(DEVICE)\n",
    "\n",
    "                    # Concatenate context\n",
    "                    context = torch.cat([context_temp, context_micro], dim=2)\n",
    "\n",
    "                    # Forward pass\n",
    "                    pred_micro = model(context, future_temp)\n",
    "\n",
    "                    # Calculate MSE loss (on valid pixels only)\n",
    "                    mask_expanded = mask.unsqueeze(1).expand_as(target_micro)\n",
    "                    mse_loss = mse_loss_fn(pred_micro[mask_expanded], target_micro[mask_expanded])\n",
    "\n",
    "                    # Calculate solidification loss\n",
    "                    _, solid_loss, _ = solidification_loss_fn(pred_micro, target_micro, future_temp, mask)\n",
    "\n",
    "                    total_mse_loss += mse_loss.item()\n",
    "                    total_solid_loss += solid_loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "            avg_mse_loss = total_mse_loss / num_batches\n",
    "            avg_solid_loss = total_solid_loss / num_batches\n",
    "\n",
    "            test_mse_losses[config['name']] = avg_mse_loss\n",
    "            test_solid_losses[config['name']] = avg_solid_loss\n",
    "\n",
    "            print(f\"    ✓ Test MSE: {avg_mse_loss:.6f}, Test Solid: {avg_solid_loss:.6f}\")\n",
    "\n",
    "    # Print summary table with accuracy (1 - loss)\n",
    "    print(\"\\n\" + \"=\"*170)\n",
    "    print(\"MODEL COMPARISON SUMMARY (Accuracy = 1 - Loss)\")\n",
    "    print(\"=\"*170)\n",
    "    print(f\"{'ID':<4} {'Name':<50} {'Train':<10} {'Val Global':<12} {'Val Solid':<12} {'Test Global':<12} {'Test Solid':<12} {'Epochs':<8}\")\n",
    "    print(\"-\"*170)\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        name = config['name']\n",
    "        if name in histories:\n",
    "            history = histories[name]\n",
    "            final_train = history['train_loss'][-1]\n",
    "            final_val = history['val_loss'][-1]\n",
    "            epochs = len(history['train_loss'])\n",
    "\n",
    "            # Convert to accuracy\n",
    "            train_acc = 1.0 - final_train\n",
    "            val_global_acc = 1.0 - final_val\n",
    "\n",
    "            # Val solidification loss\n",
    "            if config['loss_type'] == 'combined' and 'val_solidification_loss' in history:\n",
    "                final_val_solid = history['val_solidification_loss'][-1]\n",
    "                val_solid_acc = 1.0 - final_val_solid\n",
    "            elif config['loss_type'] == 'mse' and name in mse_solidification_losses:\n",
    "                final_val_solid = mse_solidification_losses[name]\n",
    "                val_solid_acc = 1.0 - final_val_solid\n",
    "            else:\n",
    "                val_solid_acc = None\n",
    "\n",
    "            # Test losses -> accuracy\n",
    "            test_mse = test_mse_losses.get(name, None)\n",
    "            test_solid = test_solid_losses.get(name, None)\n",
    "\n",
    "            test_global_acc = 1.0 - test_mse if test_mse is not None else None\n",
    "            test_solid_acc = 1.0 - test_solid if test_solid is not None else None\n",
    "\n",
    "            # Format output\n",
    "            val_solid_str = f\"{val_solid_acc:.6f}\" if val_solid_acc is not None else \"N/A\"\n",
    "            test_global_str = f\"{test_global_acc:.6f}\" if test_global_acc is not None else \"N/A\"\n",
    "            test_solid_str = f\"{test_solid_acc:.6f}\" if test_solid_acc is not None else \"N/A\"\n",
    "\n",
    "            print(f\"{config['id']:<4} {name:<50} {train_acc:<10.6f} {val_global_acc:<12.6f} {val_solid_str:<12} {test_global_str:<12} {test_solid_str:<12} {epochs:<8}\")\n",
    "        else:\n",
    "            print(f\"{config['id']:<4} {name:<50} {'N/A':<10} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'N/A':<8}\")\n",
    "\n",
    "    print(\"=\"*170)\n",
    "else:\n",
    "    print(\"No training histories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All 10 models have been trained and evaluated!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Autoregressive Temperature Prediction Demo\n",
    "\n",
    "This notebook demonstrates multi-step autoregressive prediction using the trained CNN-LSTM model.\n",
    "\n",
    "**Approach:**\n",
    "1. Load the trained model from `runs/2025-11-19_18-49-53/checkpoints/best_model.pt`\n",
    "2. Load ground truth frames from the test dataset\n",
    "3. Use first 3 frames as initial context\n",
    "4. Predict frame 4 using frames [1, 2, 3]\n",
    "5. Predict frame 5 using frames [2, 3, 4_predicted]\n",
    "6. Compare predictions vs ground truth and visualize error accumulation\n",
    "\n",
    "**Note:** Limited to 5 total frames (3 context + 2 predictions) due to test set size constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file in project root\n",
    "project_root = Path(\"../\").resolve()\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    load_dotenv(dotenv_path=env_file, override=True)\n",
    "    print(f\"Loaded .env from: {env_file}\")\n",
    "else:\n",
    "    print(f\"Warning: .env file not found at {env_file}\")\n",
    "\n",
    "# Add project root to path\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from lasernet.model.CNN_LSTM import CNN_LSTM\n",
    "from lasernet.dataset import SliceSequenceDataset\n",
    "\n",
    "data_dir = os.environ['BLACKHOLE']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "run_dir = Path(\"../runs/2025-11-19_18-49-53/\")\n",
    "checkpoint = torch.load(run_dir / \"checkpoints\" / \"best_model.pt\", map_location=device)\n",
    "config = json.loads((run_dir / \"config.json\").read_text())\n",
    "\n",
    "print(\"Checkpoint contents:\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Train loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"  Val loss: {checkpoint['val_loss']:.4f}\")\n",
    "print(f\"  split_ratio: {config['dataset']['train_sequences']}\") # Todo\n",
    "\n",
    "# Initialize model with same architecture\n",
    "model = CNN_LSTM(\n",
    "    input_channels=1,\n",
    "    hidden_channels=[16, 32, 64],\n",
    "    lstm_hidden=64,\n",
    "    lstm_layers=1,\n",
    "    temp_min=300.0,\n",
    "    temp_max=2000.0,\n",
    ").to(device)\n",
    "\n",
    "# Load trained weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"  Parameters: {model.count_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset\n",
    "\n",
    "We'll load a sequence of frames from the test set to use as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset (using same configuration as training)\n",
    "test_dataset = SliceSequenceDataset(\n",
    "    field=\"temperature\",\n",
    "    plane=\"xz\",\n",
    "    split=\"test\",\n",
    "    sequence_length=3,\n",
    "    target_offset=1,\n",
    "    preload=False,  # Faster for demo\n",
    "    train_ratio=12/24,\n",
    "    val_ratio=6/24,\n",
    "    test_ratio=6/24,\n",
    ")\n",
    "\n",
    "print(f\"Test dataset loaded:\")\n",
    "print(f\"  Total samples: {len(test_dataset)}\")\n",
    "print(f\"  Valid sequences: {test_dataset.num_valid_sequences}\")\n",
    "print(f\"  Slices per timestep: {len(test_dataset.slice_coords)}\")\n",
    "print()\n",
    "\n",
    "# Get first sample to check dimensions\n",
    "sample = test_dataset[0]\n",
    "print(f\"Sample dimensions:\")\n",
    "print(f\"  Context: {sample['context'].shape}\")\n",
    "print(f\"  Target: {sample['target'].shape}\")\n",
    "print(f\"  Timesteps: {sample['timestep_start']} -> {sample['target_timestep']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 3. Prepare Ground Truth Sequence\n",
    "\n",
    "We need to extract consecutive frames from the same slice coordinate to form a continuous sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_initial_frames = 3   # Use first 3 frames as context (matches sequence_length)\n",
    "num_predictions = 3      # Predict next 3 frames autoregressively (limited by test set size)\n",
    "total_frames_needed = num_initial_frames + num_predictions\n",
    "\n",
    "print(f\"Loading {total_frames_needed} consecutive frames...\")\n",
    "print(f\"Test set has {test_dataset.num_valid_sequences} valid sequences\")\n",
    "\n",
    "# Load temperature data directly from preprocessed file\n",
    "import os\n",
    "data_dir = os.environ['BLACKHOLE']\n",
    "temp_path = os.path.join(data_dir, 'processed', 'temperature.pt')\n",
    "\n",
    "if os.path.exists(temp_path):\n",
    "    print(f\"Loading from preprocessed file: {temp_path}\")\n",
    "    temp_data = torch.load(temp_path, map_location='cpu')  # [T, X, Y, Z]\n",
    "    print(f\"Temperature data shape: {temp_data.shape}\")\n",
    "    \n",
    "    # Get test timestep range\n",
    "    num_timesteps = temp_data.shape[0]\n",
    "    test_start_idx = int((12/24) * num_timesteps) + int((6/24) * num_timesteps)\n",
    "    test_end_idx = num_timesteps\n",
    "    \n",
    "    print(f\"Test timesteps: {test_start_idx} to {test_end_idx-1}\")\n",
    "    print(f\"Available test timesteps: {test_end_idx - test_start_idx}\")\n",
    "    \n",
    "    # Extract XZ slice (same plane as training)\n",
    "    # For XZ plane, we iterate over Y coordinates\n",
    "    slice_idx = temp_data.shape[2] // 2  # Middle Y coordinate\n",
    "    \n",
    "    # Get consecutive frames\n",
    "    ground_truth_frames = []\n",
    "    for t_idx in range(test_start_idx, min(test_start_idx + total_frames_needed, test_end_idx)):\n",
    "        # Extract XZ slice at this timestep [X, Z]\n",
    "        slice_2d = temp_data[t_idx, :, slice_idx, :]  # [X, Z]\n",
    "        \n",
    "        # Downsample by factor of 2 (same as dataset)\n",
    "        slice_2d = slice_2d[::2, ::2]\n",
    "        \n",
    "        # Add channel dimension [1, X, Z]\n",
    "        slice_2d = slice_2d.unsqueeze(0)\n",
    "        ground_truth_frames.append(slice_2d)\n",
    "    \n",
    "    # Stack into tensor [T, 1, X, Z]\n",
    "    ground_truth = torch.stack(ground_truth_frames, dim=0)\n",
    "    \n",
    "    print(f\"\\nGround truth sequence shape: {ground_truth.shape}\")\n",
    "    print(f\"Temperature range: [{ground_truth.min():.1f}, {ground_truth.max():.1f}] K\")\n",
    "    print(f\"Frames loaded: {len(ground_truth_frames)}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"ERROR: Preprocessed file not found at {temp_path}\")\n",
    "    print(\"Please run scripts/preprocess_data.py first!\")\n",
    "    raise FileNotFoundError(f\"Missing {temp_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 4. Autoregressive Prediction\n",
    "\n",
    "Use the first 3 frames as initial context, then predict frames 4-5 autoregressively:\n",
    "- Frame 4: predict using frames [1, 2, 3]\n",
    "- Frame 5: predict using frames [2, 3, 4_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize predictions list with ground truth initial frames\n",
    "predictions = [ground_truth[i] for i in range(num_initial_frames)]\n",
    "\n",
    "print(f\"Starting autoregressive prediction...\")\n",
    "print(f\"Initial frames: 0-{num_initial_frames-1}\")\n",
    "print(f\"Predicting frames: {num_initial_frames}-{num_initial_frames + num_predictions - 1}\")\n",
    "print()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for step in range(num_predictions):\n",
    "        # Get last 3 frames as input (sequence_length=3)\n",
    "        input_frames = predictions[-3:]  # List of 3 tensors [1, H, W]\n",
    "        input_seq = torch.stack(input_frames, dim=0)  # [3, 1, H, W]\n",
    "        \n",
    "        # Add batch dimension and move to device\n",
    "        input_batch = input_seq.unsqueeze(0).to(device)  # [1, 3, 1, H, W]\n",
    "        \n",
    "        # Predict next frame\n",
    "        pred = model(input_batch)  # [1, 1, H, W]\n",
    "        \n",
    "        # Remove batch dimension and move to CPU\n",
    "        pred_frame = pred.squeeze(0).cpu()  # [1, H, W]\n",
    "        \n",
    "        # Append to predictions\n",
    "        predictions.append(pred_frame)\n",
    "        \n",
    "        frame_idx = num_initial_frames + step\n",
    "        gt_frame = ground_truth[frame_idx]\n",
    "        mse = ((pred_frame - gt_frame) ** 2).mean().item()\n",
    "        mae = (pred_frame - gt_frame).abs().mean().item()\n",
    "        \n",
    "        print(f\"Frame {frame_idx}: MSE = {mse:.2f} K², MAE = {mae:.2f} K\")\n",
    "\n",
    "# Stack predictions into tensor [T, 1, H, W]\n",
    "predictions_tensor = torch.stack(predictions, dim=0)\n",
    "\n",
    "print()\n",
    "print(f\"Predicted sequence shape: {predictions_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 5. Visualize Results\n",
    "\n",
    "Plot ground truth, predictions, and error for all frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with 3 rows: ground truth, predictions, absolute error\n",
    "num_frames_to_plot = total_frames_needed\n",
    "fig, axes = plt.subplots(3, num_frames_to_plot, figsize=(2.5 * num_frames_to_plot, 8))\n",
    "\n",
    "cmap = colormaps['hot']\n",
    "vmin, vmax = 300, 4500  # Temperature range in Kelvin\n",
    "\n",
    "for i in range(num_frames_to_plot):\n",
    "    # Ground truth\n",
    "    gt = ground_truth[i, 0].cpu().numpy()  # [H, W]\n",
    "    im0 = axes[0, i].imshow(gt, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "    axes[0, i].set_title(f\"Frame {i}\\n(Ground Truth)\", fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    pred = predictions_tensor[i, 0].cpu().numpy()  # [H, W]\n",
    "    im1 = axes[1, i].imshow(pred, cmap=cmap, vmin=vmin, vmax=vmax, aspect='auto')\n",
    "    \n",
    "    if i < num_initial_frames:\n",
    "        axes[1, i].set_title(f\"Frame {i}\\n(Initial Context)\", fontsize=10)\n",
    "    else:\n",
    "        axes[1, i].set_title(f\"Frame {i}\\n(Predicted)\", fontsize=10)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Absolute error\n",
    "    error = np.abs(pred - gt)\n",
    "    im2 = axes[2, i].imshow(error, cmap='viridis', vmin=0, vmax=100, aspect='auto')\n",
    "    mae = error.mean()\n",
    "    axes[2, i].set_title(f\"MAE: {mae:.1f} K\", fontsize=10)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "# Add colorbars\n",
    "fig.colorbar(im0, ax=axes[0, :], orientation='horizontal', pad=0.02, label='Temperature (K)')\n",
    "fig.colorbar(im1, ax=axes[1, :], orientation='horizontal', pad=0.02, label='Temperature (K)')\n",
    "fig.colorbar(im2, ax=axes[2, :], orientation='horizontal', pad=0.02, label='Absolute Error (K)')\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('../runs/2025-11-19_18-49-53/visualizations/autoregressive_prediction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization saved to: ../runs/2025-11-19_18-49-53/visualizations/autoregressive_prediction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Analyze how error accumulates during autoregressive prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for each frame\n",
    "mse_per_frame = []\n",
    "mae_per_frame = []\n",
    "\n",
    "for i in range(total_frames_needed):\n",
    "    gt = ground_truth[i, 0]\n",
    "    pred = predictions_tensor[i, 0]\n",
    "    \n",
    "    mse = ((pred - gt) ** 2).mean().item()\n",
    "    mae = (pred - gt).abs().mean().item()\n",
    "    \n",
    "    mse_per_frame.append(mse)\n",
    "    mae_per_frame.append(mae)\n",
    "\n",
    "# Plot error evolution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# MSE plot\n",
    "ax1.plot(range(total_frames_needed), mse_per_frame, marker='o', linewidth=2)\n",
    "ax1.axvline(x=num_initial_frames - 0.5, color='red', linestyle='--', label='Start of predictions')\n",
    "ax1.set_xlabel('Frame Index', fontsize=12)\n",
    "ax1.set_ylabel('MSE (K²)', fontsize=12)\n",
    "ax1.set_title('Mean Squared Error vs Frame Index', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# MAE plot\n",
    "ax2.plot(range(total_frames_needed), mae_per_frame, marker='o', linewidth=2, color='orange')\n",
    "ax2.axvline(x=num_initial_frames - 0.5, color='red', linestyle='--', label='Start of predictions')\n",
    "ax2.set_xlabel('Frame Index', fontsize=12)\n",
    "ax2.set_ylabel('MAE (K)', fontsize=12)\n",
    "ax2.set_title('Mean Absolute Error vs Frame Index', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../runs/2025-11-19_18-49-53/visualizations/error_evolution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nError Statistics:\")\n",
    "print(f\"  Initial context (frames 0-{num_initial_frames-1}): MAE = {np.mean(mae_per_frame[:num_initial_frames]):.2f} K (should be ~0)\")\n",
    "print(f\"  Predicted frames ({num_initial_frames}-{total_frames_needed-1}): MAE = {np.mean(mae_per_frame[num_initial_frames:]):.2f} K\")\n",
    "print(f\"  Error increase per step: {(mae_per_frame[-1] - mae_per_frame[num_initial_frames]) / num_predictions:.2f} K/step\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
