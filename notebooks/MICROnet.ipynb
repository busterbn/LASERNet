{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICROnet Training - Comprehensive Model Comparison\n",
    "\n",
    "This notebook trains 16 different microstructure prediction models using U-Net architecture with skip connections:\n",
    "\n",
    "## CNN-LSTM Models (8 models):\n",
    "### MSE Loss:\n",
    "1. seq=2, MSE loss\n",
    "2. seq=3, MSE loss\n",
    "3. seq=4, MSE loss\n",
    "\n",
    "### Combined Loss (seq=3):\n",
    "4. T_solidus=1560, T_liquidus=1620, weights=70/30\n",
    "5. T_solidus=1530, T_liquidus=1650, weights=70/30\n",
    "6. T_solidus=1500, T_liquidus=1680, weights=70/30\n",
    "7. T_solidus=1560, T_liquidus=1620, weights=50/50\n",
    "8. T_solidus=1560, T_liquidus=1620, weights=30/70\n",
    "\n",
    "## PredRNN Models (8 models):\n",
    "### MSE Loss:\n",
    "9. seq=2, MSE loss\n",
    "10. seq=3, MSE loss\n",
    "11. seq=4, MSE loss\n",
    "\n",
    "### Combined Loss (seq=2):\n",
    "12. T_solidus=1560, T_liquidus=1620, weights=70/30\n",
    "13. T_solidus=1530, T_liquidus=1650, weights=70/30\n",
    "14. T_solidus=1500, T_liquidus=1680, weights=70/30\n",
    "15. T_solidus=1560, T_liquidus=1620, weights=50/50\n",
    "16. T_solidus=1560, T_liquidus=1620, weights=30/70\n",
    "\n",
    "All models use U-Net architecture with skip connections for improved gradient flow and feature reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path so we can import lasernet\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    print(f\"Added {project_root} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from lasernet.micronet.train.trainer import (\n",
    "    get_device,\n",
    "    train_microstructure,\n",
    "    load_model_and_predict,\n",
    "    load_model_and_predict_cascaded,\n",
    "    save_prediction_visualization,\n",
    "    save_solidification_mask_visualization,\n",
    "    save_cascaded_prediction_visualization,\n",
    ")\n",
    "from lasernet.micronet.model.MicrostructureCNN_LSTM import MicrostructureCNN_LSTM\n",
    "from lasernet.micronet.model.MicrostructurePredRNN import MicrostructurePredRNN\n",
    "from lasernet.micronet.model.losses import CombinedLoss\n",
    "from lasernet.micronet.utils import plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds for Reproducibility\n",
    "\n",
    "Set all random seeds to ensure reproducible results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Set Python random seed\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set NumPy random seed\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Set PyTorch random seeds\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # For multi-GPU setups\n",
    "\n",
    "# Configure PyTorch to use deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set environment variable for additional determinism\n",
    "os.environ['PYTHONHASHSEED'] = str(RANDOM_SEED)\n",
    "\n",
    "print(f\"Random seed set to {RANDOM_SEED} for reproducibility\")\n",
    "print(\"  ✓ Python random\")\n",
    "print(\"  ✓ NumPy\")\n",
    "print(\"  ✓ PyTorch (CPU and CUDA)\")\n",
    "print(\"  ✓ cuDNN deterministic mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file in project root\n",
    "project_root = Path.cwd().parent\n",
    "env_file = project_root / \".env\"\n",
    "if env_file.exists():\n",
    "    load_dotenv(dotenv_path=env_file, override=True)\n",
    "    print(f\"Loaded .env from: {env_file}\")\n",
    "else:\n",
    "    print(f\"Warning: .env file not found at {env_file}\")\n",
    "\n",
    "# Verify BLACKHOLE is set\n",
    "blackhole_path = os.environ.get(\"BLACKHOLE\")\n",
    "if blackhole_path:\n",
    "    print(f\"BLACKHOLE environment variable: {blackhole_path}\")\n",
    "else:\n",
    "    raise ValueError(\"BLACKHOLE environment variable not set. Please create a .env file with BLACKHOLE=/path/to/data\")\n",
    "\n",
    "# Global configuration\n",
    "DEVICE = get_device()\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "PATIENCE = 25\n",
    "OUTPUT_DIR = Path(\"MICROnet_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Prediction settings\n",
    "PRED_TIMESTEP = 23\n",
    "PRED_SLICE = 47\n",
    "PLANE = \"xz\"\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Prediction settings: timestep={PRED_TIMESTEP}, slice={PRED_SLICE}, plane={PRED_TIMESTEP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "Load datasets using fast loading from preprocessed files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets (same approach as notebook.ipynb)\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "from pathlib import Path as PathLib\n",
    "from lasernet.micronet.dataset import MicrostructureSequenceDataset\n",
    "from lasernet.micronet.dataset.fast_loading import FastMicrostructureSequenceDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "SEQ_LENGTH = 4  # Default sequence length for dataset creation\n",
    "SPLIT_RATIO = \"12,6,6\"\n",
    "\n",
    "# Parse split ratios\n",
    "split_ratios = list(map(int, SPLIT_RATIO.split(\",\")))\n",
    "train_ratio = split_ratios[0] / sum(split_ratios)\n",
    "val_ratio = split_ratios[1] / sum(split_ratios)\n",
    "test_ratio = split_ratios[2] / sum(split_ratios)\n",
    "\n",
    "# Check if preprocessed files are available for fast loading\n",
    "blackhole = os.environ.get(\"BLACKHOLE\")\n",
    "if not blackhole:\n",
    "    raise ValueError(\"BLACKHOLE environment variable not set. Please set it in the makefile or shell.\")\n",
    "\n",
    "print(f\"BLACKHOLE directory: {blackhole}\")\n",
    "\n",
    "processed_dir = PathLib(blackhole) / \"processed\" / \"data\"\n",
    "required_files = [\"coordinates.pt\", \"microstructure.pt\", \"temperature.pt\"]\n",
    "fast_loading_available = all((processed_dir / f).exists() for f in required_files)\n",
    "\n",
    "if fast_loading_available:\n",
    "    print(\"✓ Preprocessed files found - using fast loading\")\n",
    "    DatasetClass = FastMicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "else:\n",
    "    print(\"⚠ Preprocessed files not found - using standard loading\")\n",
    "    DatasetClass = MicrostructureSequenceDataset\n",
    "    dataset_kwargs = {\n",
    "        \"plane\": PLANE,\n",
    "        \"split\": \"train\",  # will be overridden for each dataset\n",
    "        \"sequence_length\": SEQ_LENGTH,\n",
    "        \"target_offset\": 1,\n",
    "        \"preload\": True,\n",
    "        \"train_ratio\": train_ratio,\n",
    "        \"val_ratio\": val_ratio,\n",
    "        \"test_ratio\": test_ratio,\n",
    "    }\n",
    "\n",
    "train_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"train\"})\n",
    "val_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"val\"})\n",
    "test_dataset = DatasetClass(**{**dataset_kwargs, \"split\": \"test\"})\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples:   {len(val_dataset)}\")\n",
    "print(f\"  Test samples:  {len(test_dataset)}\")\n",
    "\n",
    "# Show sample dimensions\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample dimensions:\")\n",
    "print(f\"  Context temp:  {sample['context_temp'].shape}\")\n",
    "print(f\"  Context micro: {sample['context_micro'].shape}\")\n",
    "print(f\"  Future temp:   {sample['future_temp'].shape}\")\n",
    "print(f\"  Target micro:  {sample['target_micro'].shape}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "print(\"\\nCreating DataLoaders...\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"DataLoaders created with batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "Define all 10 model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all model configurations\n",
    "MODEL_CONFIGS = [\n",
    "    # CNN-LSTM models with MSE loss\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"01_MICROnet_cnn_lstm_seq2_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"02_MICROnet_cnn_lstm_seq3_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"03_MICROnet_cnn_lstm_seq4_MSEloss\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # CNN-LSTM models with Combined loss\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"04_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"name\": \"05_MICROnet_cnn_lstm_seq3_CombLoss_T1530-1650_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"name\": \"06_MICROnet_cnn_lstm_seq3_CombLoss_T1500-1680_s70_g30\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"name\": \"07_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s50_g50\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.5,\n",
    "        \"global_weight\": 0.5,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"name\": \"08_MICROnet_cnn_lstm_seq3_CombLoss_T1560-1620_s30_g70\",\n",
    "        \"model_type\": \"cnn_lstm\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.3,\n",
    "        \"global_weight\": 0.7,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # PredRNN models with MSE loss\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"name\": \"09_MICROnet_predrnn_seq2_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"name\": \"10_MICROnet_predrnn_seq3_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 3,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"name\": \"11_MICROnet_predrnn_seq4_MSEloss\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 4,\n",
    "        \"loss_type\": \"mse\",\n",
    "        \"t_solidus\": None,\n",
    "        \"t_liquidus\": None,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    # PredRNN models with Combined loss\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"name\": \"12_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"name\": \"13_MICROnet_predrnn_seq2_CombLoss_T1530-1650_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1530.0,\n",
    "        \"t_liquidus\": 1650.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"name\": \"14_MICROnet_predrnn_seq2_CombLoss_T1500-1680_s70_g30\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1500.0,\n",
    "        \"t_liquidus\": 1680.0,\n",
    "        \"solidification_weight\": 0.7,\n",
    "        \"global_weight\": 0.3,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"name\": \"15_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s50_g50\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.5,\n",
    "        \"global_weight\": 0.5,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"name\": \"16_MICROnet_predrnn_seq2_CombLoss_T1560-1620_s30_g70\",\n",
    "        \"model_type\": \"predrnn\",\n",
    "        \"seq_length\": 2,\n",
    "        \"loss_type\": \"combined\",\n",
    "        \"t_solidus\": 1560.0,\n",
    "        \"t_liquidus\": 1620.0,\n",
    "        \"solidification_weight\": 0.3,\n",
    "        \"global_weight\": 0.7,\n",
    "        \"use_skip_connections\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Configured {len(MODEL_CONFIGS)} models for training:\")\n",
    "print(f\"\\n{'ID':<4} {'Model':<10} {'Seq':<5} {'Loss':<40}\")\n",
    "print(\"-\" * 65)\n",
    "for cfg in MODEL_CONFIGS:\n",
    "    skip_str = \" (U-Net)\" if cfg.get('use_skip_connections', False) else \"\"\n",
    "    loss_str = f\"MSE\" if cfg['loss_type'] == 'mse' else f\"Combined T{cfg['t_solidus']:.0f}-{cfg['t_liquidus']:.0f}\"\n",
    "    print(f\"{cfg['id']:<4} {cfg['model_type']:<10} {cfg['seq_length']:<5} {loss_str:<40}{skip_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Train each model. Skip if already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Train a single model based on configuration.\n",
    "    Skip if already trained.\n",
    "    \"\"\"\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "    # Check if already trained\n",
    "    if checkpoint_path.exists():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Model {config['id']}: {config['name']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(\"✓ Model already trained. Skipping training.\")\n",
    "\n",
    "        # Load history if available\n",
    "        history_path = run_dir / \"history.json\"\n",
    "        if history_path.exists():\n",
    "            with open(history_path, 'r') as f:\n",
    "                history = json.load(f)\n",
    "        else:\n",
    "            history = None\n",
    "\n",
    "        return {\"status\": \"skipped\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "    # Create directories\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model {config['id']}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Model type: {config['model_type']}\")\n",
    "    print(f\"  Sequence length: {config['seq_length']}\")\n",
    "    print(f\"  Loss type: {config['loss_type']}\")\n",
    "    print(f\"  Skip connections: {config.get('use_skip_connections', False)}\")\n",
    "    if config['loss_type'] == 'combined':\n",
    "        print(f\"  T_solidus: {config['t_solidus']} K\")\n",
    "        print(f\"  T_liquidus: {config['t_liquidus']} K\")\n",
    "        print(f\"  Solidification weight: {config.get('solidification_weight', 0.7)}\")\n",
    "        print(f\"  Global weight: {config.get('global_weight', 0.3)}\")\n",
    "\n",
    "    # FIX: Create dataloaders with the CORRECT sequence length for THIS model\n",
    "    print(f\"\\n  Creating datasets with sequence length {config['seq_length']}...\")\n",
    "    if fast_loading_available:\n",
    "        model_train_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = FastMicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "    else:\n",
    "        model_train_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"train\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "        model_val_dataset = MicrostructureSequenceDataset(\n",
    "            plane=PLANE,\n",
    "            split=\"val\",\n",
    "            sequence_length=config['seq_length'],\n",
    "            target_offset=1,\n",
    "            preload=True,\n",
    "            train_ratio=train_ratio,\n",
    "            val_ratio=val_ratio,\n",
    "            test_ratio=test_ratio,\n",
    "        )\n",
    "\n",
    "    model_train_loader = DataLoader(model_train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    model_val_loader = DataLoader(model_val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"  ✓ Datasets created: train={len(model_train_dataset)}, val={len(model_val_dataset)}\")\n",
    "\n",
    "    # Create model with skip connections option\n",
    "    use_skip_connections = config.get('use_skip_connections', False)\n",
    "\n",
    "    if config['model_type'] == 'cnn_lstm':\n",
    "        model = MicrostructureCNN_LSTM(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "    else:  # predrnn\n",
    "        model = MicrostructurePredRNN(\n",
    "            input_channels=10,\n",
    "            future_channels=1,\n",
    "            output_channels=9,\n",
    "            use_skip_connections=use_skip_connections,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "    param_count = model.count_parameters()\n",
    "    print(f\"  Parameters: {param_count:,}\")\n",
    "\n",
    "    # Create loss function\n",
    "    if config['loss_type'] == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:  # combined\n",
    "        criterion = CombinedLoss(\n",
    "            solidification_weight=config.get('solidification_weight', 0.7),\n",
    "            global_weight=config.get('global_weight', 0.3),\n",
    "            T_solidus=config['t_solidus'],\n",
    "            T_liquidus=config['t_liquidus'],\n",
    "            weight_type=\"gaussian\",\n",
    "            weight_scale=0.1,\n",
    "            base_weight=0.1,\n",
    "            return_components=True,  # Enable component tracking for visualization\n",
    "        )\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Save configuration\n",
    "    config_dict = {\n",
    "        \"model\": {\n",
    "            \"name\": config['model_type'],\n",
    "            \"parameters\": param_count,\n",
    "            \"sequence_length\": config['seq_length'],\n",
    "            \"use_skip_connections\": use_skip_connections,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"loss_type\": config['loss_type'],\n",
    "        },\n",
    "        \"device\": str(DEVICE),\n",
    "    }\n",
    "\n",
    "    if config['loss_type'] == 'combined':\n",
    "        config_dict['training']['t_solidus'] = config['t_solidus']\n",
    "        config_dict['training']['t_liquidus'] = config['t_liquidus']\n",
    "        config_dict['training']['solidification_weight'] = config.get('solidification_weight', 0.7)\n",
    "        config_dict['training']['global_weight'] = config.get('global_weight', 0.3)\n",
    "\n",
    "    with open(run_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    # Train with the CORRECT dataloaders for this model's sequence length\n",
    "    print(\"\\nStarting training...\")\n",
    "    history = train_microstructure(\n",
    "        model=model,\n",
    "        train_loader=model_train_loader,\n",
    "        val_loader=model_val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=DEVICE,\n",
    "        epochs=EPOCHS,\n",
    "        run_dir=run_dir,\n",
    "        patience=PATIENCE,\n",
    "    )\n",
    "\n",
    "    # Save history\n",
    "    with open(run_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    # Plot losses\n",
    "    plot_losses(history, str(run_dir / \"training_losses.png\"))\n",
    "\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'epoch': len(history['train_loss']),\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'config': config_dict,\n",
    "        'history': history,\n",
    "    }, run_dir / \"checkpoints\" / \"final_model.pt\")\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"  Final train loss: {history['train_loss'][-1]:.6f}\")\n",
    "    print(f\"  Final val loss: {history['val_loss'][-1]:.6f}\")\n",
    "    print(f\"  Epochs: {len(history['train_loss'])}\")\n",
    "\n",
    "    return {\"status\": \"trained\", \"history\": history, \"run_dir\": run_dir}\n",
    "\n",
    "\n",
    "# Train all models\n",
    "results = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    results[config['name']] = train_model(config)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All models processed!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cascaded Predictions\n",
    "\n",
    "Generate predictions for timestep 23, slice 47 for all models using the cascaded pipeline (TempNet → MicroNet).\n",
    "\n",
    "This implements the correct theory: given previous microstructure and temperature frames, we first predict the next temperature frame using TempNet, then use that prediction (not ground truth) to predict the next microstructure frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TempNet checkpoint path - UPDATE THIS PATH TO YOUR TRAINED TEMPNET MODEL\n",
    "TEMPNET_CHECKPOINT = \"runs/2024-XX-XX_XX-XX-XX/checkpoints/best_model.pt\"  # TODO: Update this path\n",
    "\n",
    "# Check if TempNet checkpoint exists\n",
    "from pathlib import Path as PathLib\n",
    "if not PathLib(TEMPNET_CHECKPOINT).exists():\n",
    "    print(f\"⚠ WARNING: TempNet checkpoint not found at: {TEMPNET_CHECKPOINT}\")\n",
    "    print(\"Please update TEMPNET_CHECKPOINT path to your trained TempNet model\")\n",
    "    print(\"Cascaded predictions will not be generated without a trained TempNet model.\")\n",
    "    TEMPNET_AVAILABLE = False\n",
    "else:\n",
    "    print(f\"✓ TempNet checkpoint found: {TEMPNET_CHECKPOINT}\")\n",
    "    TEMPNET_AVAILABLE = True\n",
    "\n",
    "\n",
    "def generate_cascaded_prediction(config: dict) -> None:\n",
    "    \"\"\"\n",
    "    Generate cascaded prediction visualization (TempNet → MicroNet).\n",
    "    This is the theoretically correct approach.\n",
    "    \"\"\"\n",
    "    if not TEMPNET_AVAILABLE:\n",
    "        print(f\"Model {config['id']}: ✗ Skipping (TempNet not available)\")\n",
    "        return\n",
    "    \n",
    "    from lasernet.micronet.train.trainer import load_model_and_predict_cascaded, save_cascaded_prediction_visualization\n",
    "    \n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    checkpoint_path = run_dir / \"checkpoints\" / \"best_model.pt\"\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    # Check if prediction already exists\n",
    "    if pred_path.exists():\n",
    "        # For combined loss models, also check solidification mask\n",
    "        if config['loss_type'] == 'combined':\n",
    "            solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "            if solid_mask_path.exists():\n",
    "                print(f\"Model {config['id']}: ✓ Predictions already exist. Skipping.\")\n",
    "                return\n",
    "        else:\n",
    "            print(f\"Model {config['id']}: ✓ Prediction already exists. Skipping.\")\n",
    "            return\n",
    "\n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Model {config['id']}: ✗ No checkpoint found\")\n",
    "        return\n",
    "    \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Model {config['id']}: ✗ No MicroNet checkpoint found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model {config['id']}: Generating cascaded prediction...\")\n",
    "    \n",
    "    try:\n",
    "        pred_micro, target_micro, future_temp, mask, metadata = load_model_and_predict(\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            timestep=PRED_TIMESTEP,\n",
    "            slice_index=PRED_SLICE,\n",
    "            sequence_length=config['seq_length'],\n",
    "            plane=PLANE,\n",
    "            device=str(DEVICE),\n",
    "        )\n",
    "\n",
    "        # Create loss function for visualization\n",
    "        if config['loss_type'] == 'mse':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        else:  # combined\n",
    "            loss_fn = CombinedLoss(\n",
    "                solidification_weight=config.get('solidification_weight', 0.7),\n",
    "                global_weight=config.get('global_weight', 0.3),\n",
    "                T_solidus=config['t_solidus'],\n",
    "                T_liquidus=config['t_liquidus'],\n",
    "                weight_type=\"gaussian\",\n",
    "                weight_scale=0.1,\n",
    "                base_weight=0.1,\n",
    "                return_components=True,\n",
    "            )\n",
    "\n",
    "        # Generate standard prediction visualization\n",
    "        save_prediction_visualization(\n",
    "            pred_micro=pred_micro,\n",
    "            target_micro=target_micro,\n",
    "            mask=mask,\n",
    "            save_path=str(pred_path),\n",
    "            title=f\"Model {config['id']}: {config['name']}\",\n",
    "            future_temp=future_temp,\n",
    "            loss_fn=loss_fn,\n",
    "        )\n",
    "        print(f\"  ✓ Saved prediction to {pred_path}\")\n",
    "\n",
    "        # Generate solidification mask visualization for combined loss models\n",
    "        if config['loss_type'] == 'combined':\n",
    "            solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "            save_solidification_mask_visualization(\n",
    "                future_temp=pred_temp,  # Use PREDICTED temperature, not ground truth\n",
    "                pred_micro=pred_micro,\n",
    "                target_micro=target_micro,\n",
    "                mask=mask,\n",
    "                loss_fn=loss_fn,\n",
    "                save_path=str(solid_mask_path),\n",
    "                title=f\"Model {config['id']}: {config['name']} (Cascaded with Predicted Temp)\",\n",
    "                timestep=metadata['timestep'],\n",
    "                slice_coord=metadata['slice_coord'],\n",
    "            )\n",
    "            print(f\"  ✓ Saved cascaded solidification mask to {solid_mask_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "print(f\"\\nGenerating predictions for timestep={PRED_TIMESTEP}, slice={PRED_SLICE}...\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    generate_prediction(config)\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    generate_cascaded_prediction(config)\n",
    "\n",
    "print(\"\\nAll cascaded predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Cascaded Predictions\n",
    "\n",
    "Display cascaded prediction visualizations showing both TempNet and MicroNet outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cascaded Prediction Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "print(\"Shows: Ground Truth Temp → Predicted Temp → Ground Truth Micro → Predicted Micro\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    cascaded_pred_path = run_dir / f\"cascaded_pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "    \n",
    "    if cascaded_pred_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(cascaded_pred_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No cascaded prediction found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Training Losses\n",
    "\n",
    "Display training loss plots for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Loss Plots:\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    loss_plot_path = run_dir / \"training_losses.png\"\n",
    "\n",
    "    if loss_plot_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(loss_plot_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No training losses plot found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Predictions\n",
    "\n",
    "Display prediction visualizations for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Prediction Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    pred_path = run_dir / f\"pred_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if pred_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(pred_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No prediction found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Solidification Mask Visualizations\n",
    "\n",
    "Display solidification mask visualizations for models trained with combined loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Solidification Mask Visualizations (t={PRED_TIMESTEP}, s={PRED_SLICE}):\\n\")\n",
    "print(\"(Only for models trained with combined loss)\\n\")\n",
    "\n",
    "for config in MODEL_CONFIGS:\n",
    "    # Skip non-combined loss models\n",
    "    if config['loss_type'] != 'combined':\n",
    "        continue\n",
    "\n",
    "    run_dir = OUTPUT_DIR / config['name']\n",
    "    solid_mask_path = run_dir / f\"solidification_mask_t{PRED_TIMESTEP}_s{PRED_SLICE}.png\"\n",
    "\n",
    "    if solid_mask_path.exists():\n",
    "        print(f\"\\nModel {config['id']}: {config['name']}\")\n",
    "        display(Image(filename=str(solid_mask_path)))\n",
    "    else:\n",
    "        print(f\"\\nModel {config['id']}: No solidification mask found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare training losses across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all histories\n",
    "histories = {}\n",
    "for config in MODEL_CONFIGS:\n",
    "    history_path = OUTPUT_DIR / config['name'] / \"history.json\"\n",
    "    if history_path.exists():\n",
    "        with open(history_path, 'r') as f:\n",
    "            histories[config['name']] = json.load(f)\n",
    "\n",
    "if histories:\n",
    "    # Plot comparison\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Training loss\n",
    "    for name, history in histories.items():\n",
    "        ax1.plot(history['train_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Validation loss\n",
    "    for name, history in histories.items():\n",
    "        ax2.plot(history['val_loss'], label=name, linewidth=2, alpha=0.7)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Validation Loss', fontsize=12)\n",
    "    ax2.set_title('Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    comparison_path = OUTPUT_DIR / \"all_models_comparison.png\"\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nComparison plot saved to {comparison_path}\")\n",
    "\n",
    "    # Calculate solidification loss for MSE models on validation set\n",
    "    print(\"\\nCalculating solidification loss for MSE models on validation set...\")\n",
    "\n",
    "    # Create a CombinedLoss function for solidification region evaluation (T_solidus=1560, T_liquidus=1620)\n",
    "    solidification_loss_fn = CombinedLoss(\n",
    "        solidification_weight=1.0,\n",
    "        global_weight=0.0,\n",
    "        T_solidus=1560.0,\n",
    "        T_liquidus=1620.0,\n",
    "        weight_type=\"gaussian\",\n",
    "        weight_scale=0.1,\n",
    "        base_weight=0.1,\n",
    "        return_components=True,\n",
    "    )\n",
    "\n",
    "    mse_solidification_losses = {}\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        if config['loss_type'] == 'mse':\n",
    "            checkpoint_path = OUTPUT_DIR / config['name'] / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "            if checkpoint_path.exists():\n",
    "                print(f\"  Model {config['id']}: Computing solidification loss...\")\n",
    "\n",
    "                # Load model\n",
    "                checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "                if config['model_type'] == 'cnn_lstm':\n",
    "                    model = MicrostructureCNN_LSTM(\n",
    "                        input_channels=10,\n",
    "                        future_channels=1,\n",
    "                        output_channels=9,\n",
    "                        use_skip_connections=config.get('use_skip_connections', False),\n",
    "                    ).to(DEVICE)\n",
    "                else:  # predrnn\n",
    "                    model = MicrostructurePredRNN(\n",
    "                        input_channels=10,\n",
    "                        future_channels=1,\n",
    "                        output_channels=9,\n",
    "                        use_skip_connections=config.get('use_skip_connections', False),\n",
    "                    ).to(DEVICE)\n",
    "\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model.eval()\n",
    "\n",
    "                # Create validation dataset with correct sequence length\n",
    "                if fast_loading_available:\n",
    "                    eval_dataset = FastMicrostructureSequenceDataset(\n",
    "                        plane=PLANE,\n",
    "                        split=\"val\",\n",
    "                        sequence_length=config['seq_length'],\n",
    "                        target_offset=1,\n",
    "                        train_ratio=train_ratio,\n",
    "                        val_ratio=val_ratio,\n",
    "                        test_ratio=test_ratio,\n",
    "                    )\n",
    "                else:\n",
    "                    eval_dataset = MicrostructureSequenceDataset(\n",
    "                        plane=PLANE,\n",
    "                        split=\"val\",\n",
    "                        sequence_length=config['seq_length'],\n",
    "                        target_offset=1,\n",
    "                        preload=True,\n",
    "                        train_ratio=train_ratio,\n",
    "                        val_ratio=val_ratio,\n",
    "                        test_ratio=test_ratio,\n",
    "                    )\n",
    "\n",
    "                eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                total_solid_loss = 0.0\n",
    "                num_batches = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for batch in eval_loader:\n",
    "                        context_temp = batch['context_temp'].to(DEVICE)\n",
    "                        context_micro = batch['context_micro'].to(DEVICE)\n",
    "                        future_temp = batch['future_temp'].to(DEVICE)\n",
    "                        target_micro = batch['target_micro'].to(DEVICE)\n",
    "                        mask = batch['target_mask'].to(DEVICE)\n",
    "\n",
    "                        # Concatenate context temperature and microstructure\n",
    "                        context = torch.cat([context_temp, context_micro], dim=2)\n",
    "\n",
    "                        # Forward pass\n",
    "                        pred_micro = model(context, future_temp)\n",
    "\n",
    "                        # Calculate solidification loss\n",
    "                        total_loss, solid_loss, global_loss = solidification_loss_fn(pred_micro, target_micro, future_temp, mask)\n",
    "\n",
    "                        total_solid_loss += solid_loss.item()\n",
    "                        num_batches += 1\n",
    "\n",
    "                avg_solid_loss = total_solid_loss / num_batches\n",
    "                mse_solidification_losses[config['name']] = avg_solid_loss\n",
    "                print(f\"    ✓ Solidification loss: {avg_solid_loss:.6f}\")\n",
    "\n",
    "    # Calculate test set metrics for all models\n",
    "    print(\"\\nCalculating test set metrics for all models...\")\n",
    "\n",
    "    test_mse_losses = {}\n",
    "    test_solid_losses = {}\n",
    "\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        checkpoint_path = OUTPUT_DIR / config['name'] / \"checkpoints\" / \"best_model.pt\"\n",
    "\n",
    "        if checkpoint_path.exists():\n",
    "            print(f\"  Model {config['id']}: Computing test set metrics...\")\n",
    "\n",
    "            # Load model\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "            if config['model_type'] == 'cnn_lstm':\n",
    "                model = MicrostructureCNN_LSTM(\n",
    "                    input_channels=10,\n",
    "                    future_channels=1,\n",
    "                    output_channels=9,\n",
    "                    use_skip_connections=config.get('use_skip_connections', False),\n",
    "                ).to(DEVICE)\n",
    "            else:  # predrnn\n",
    "                model = MicrostructurePredRNN(\n",
    "                    input_channels=10,\n",
    "                    future_channels=1,\n",
    "                    output_channels=9,\n",
    "                    use_skip_connections=config.get('use_skip_connections', False),\n",
    "                ).to(DEVICE)\n",
    "\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "\n",
    "            # Create test dataset with correct sequence length\n",
    "            if fast_loading_available:\n",
    "                test_dataset = FastMicrostructureSequenceDataset(\n",
    "                    plane=PLANE,\n",
    "                    split=\"test\",\n",
    "                    sequence_length=config['seq_length'],\n",
    "                    target_offset=1,\n",
    "                    train_ratio=train_ratio,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio,\n",
    "                )\n",
    "            else:\n",
    "                test_dataset = MicrostructureSequenceDataset(\n",
    "                    plane=PLANE,\n",
    "                    split=\"test\",\n",
    "                    sequence_length=config['seq_length'],\n",
    "                    target_offset=1,\n",
    "                    preload=True,\n",
    "                    train_ratio=train_ratio,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio,\n",
    "                )\n",
    "\n",
    "            test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            total_mse_loss = 0.0\n",
    "            total_solid_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    context_temp = batch['context_temp'].to(DEVICE)\n",
    "                    context_micro = batch['context_micro'].to(DEVICE)\n",
    "                    future_temp = batch['future_temp'].to(DEVICE)\n",
    "                    target_micro = batch['target_micro'].to(DEVICE)\n",
    "                    mask = batch['target_mask'].to(DEVICE)\n",
    "\n",
    "                    # Concatenate context\n",
    "                    context = torch.cat([context_temp, context_micro], dim=2)\n",
    "\n",
    "                    # Forward pass\n",
    "                    pred_micro = model(context, future_temp)\n",
    "\n",
    "                    # Calculate MSE loss (on valid pixels only)\n",
    "                    mask_expanded = mask.unsqueeze(1).expand_as(target_micro)\n",
    "                    mse_loss = mse_loss_fn(pred_micro[mask_expanded], target_micro[mask_expanded])\n",
    "\n",
    "                    # Calculate solidification loss\n",
    "                    _, solid_loss, _ = solidification_loss_fn(pred_micro, target_micro, future_temp, mask)\n",
    "\n",
    "                    total_mse_loss += mse_loss.item()\n",
    "                    total_solid_loss += solid_loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "            avg_mse_loss = total_mse_loss / num_batches\n",
    "            avg_solid_loss = total_solid_loss / num_batches\n",
    "\n",
    "            test_mse_losses[config['name']] = avg_mse_loss\n",
    "            test_solid_losses[config['name']] = avg_solid_loss\n",
    "\n",
    "            print(f\"    ✓ Test MSE: {avg_mse_loss:.6f}, Test Solid: {avg_solid_loss:.6f}\")\n",
    "\n",
    "    # Print summary table with accuracy (1 - loss)\n",
    "    print(\"\\n\" + \"=\"*170)\n",
    "    print(\"MODEL COMPARISON SUMMARY (Accuracy = 1 - Loss)\")\n",
    "    print(\"=\"*170)\n",
    "    print(f\"{'ID':<4} {'Name':<50} {'Train':<10} {'Val Global':<12} {'Val Solid':<12} {'Test Global':<12} {'Test Solid':<12} {'Epochs':<8}\")\n",
    "    print(\"-\"*170)\n",
    "\n",
    "    for config in MODEL_CONFIGS:\n",
    "        name = config['name']\n",
    "        if name in histories:\n",
    "            history = histories[name]\n",
    "            final_train = history['train_loss'][-1]\n",
    "            final_val = history['val_loss'][-1]\n",
    "            epochs = len(history['train_loss'])\n",
    "\n",
    "            # Convert to accuracy\n",
    "            train_acc = 1.0 - final_train\n",
    "            val_global_acc = 1.0 - final_val\n",
    "\n",
    "            # Val solidification loss\n",
    "            if config['loss_type'] == 'combined' and 'val_solidification_loss' in history:\n",
    "                final_val_solid = history['val_solidification_loss'][-1]\n",
    "                val_solid_acc = 1.0 - final_val_solid\n",
    "            elif config['loss_type'] == 'mse' and name in mse_solidification_losses:\n",
    "                final_val_solid = mse_solidification_losses[name]\n",
    "                val_solid_acc = 1.0 - final_val_solid\n",
    "            else:\n",
    "                val_solid_acc = None\n",
    "\n",
    "            # Test losses -> accuracy\n",
    "            test_mse = test_mse_losses.get(name, None)\n",
    "            test_solid = test_solid_losses.get(name, None)\n",
    "\n",
    "            test_global_acc = 1.0 - test_mse if test_mse is not None else None\n",
    "            test_solid_acc = 1.0 - test_solid if test_solid is not None else None\n",
    "\n",
    "            # Format output\n",
    "            val_solid_str = f\"{val_solid_acc:.6f}\" if val_solid_acc is not None else \"N/A\"\n",
    "            test_global_str = f\"{test_global_acc:.6f}\" if test_global_acc is not None else \"N/A\"\n",
    "            test_solid_str = f\"{test_solid_acc:.6f}\" if test_solid_acc is not None else \"N/A\"\n",
    "\n",
    "            print(f\"{config['id']:<4} {name:<50} {train_acc:<10.6f} {val_global_acc:<12.6f} {val_solid_str:<12} {test_global_str:<12} {test_solid_str:<12} {epochs:<8}\")\n",
    "        else:\n",
    "            print(f\"{config['id']:<4} {name:<50} {'N/A':<10} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'N/A':<12} {'N/A':<8}\")\n",
    "\n",
    "    print(\"=\"*170)\n",
    "else:\n",
    "    print(\"No training histories found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All 10 models have been trained and evaluated!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lasernet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
